#ifndef __KS_TASKH_H
#define __KS_TASKH_H

/* Generated by kstructs */
#include <stddef.h>
#include <stdint.h>
#include <stdbool.h>

struct ks_dyld_uuid_info_64;
typedef struct ks_dyld_uuid_info_64 ks_dyld_uuid_info_64;
struct ks_internal_state;
typedef struct ks_internal_state ks_internal_state;
struct ks_ipc_conn_port_label;
typedef struct ks_ipc_conn_port_label ks_ipc_conn_port_label;
struct ks_ipc_entry_table;
typedef struct ks_ipc_entry_table ks_ipc_entry_table;
struct ks_ipc_kobject_label;
typedef struct ks_ipc_kobject_label ks_ipc_kobject_label;
struct ks_ipc_port_request_table;
typedef struct ks_ipc_port_request_table ks_ipc_port_request_table;
struct ks_ipc_service_port_label;
typedef struct ks_ipc_service_port_label ks_ipc_service_port_label;
struct ks_knote;
typedef struct ks_knote ks_knote;
struct ks_ledger_template;
typedef struct ks_ledger_template ks_ledger_template;
struct ks_mk_timer;
typedef struct ks_mk_timer ks_mk_timer;
struct ks_pmap;
typedef struct ks_pmap ks_pmap;
struct ks_priority_queue_entry_sched;
typedef struct ks_priority_queue_entry_sched ks_priority_queue_entry_sched;
struct ks_proc;
typedef struct ks_proc ks_proc;
struct ks_pset_node;
typedef struct ks_pset_node ks_pset_node;
struct ks_thread;
typedef struct ks_thread ks_thread;
struct ks_thread_call;
typedef struct ks_thread_call ks_thread_call;
struct ks_thread_group;
typedef struct ks_thread_group ks_thread_group;
struct ks_turnstile;
typedef struct ks_turnstile ks_turnstile;
struct ks_ucred;
typedef struct ks_ucred ks_ucred;
struct ks_vm_map_corpse_footprint_header;
typedef struct ks_vm_map_corpse_footprint_header ks_vm_map_corpse_footprint_header;
struct ks_vm_map_entry;
typedef struct ks_vm_map_entry ks_vm_map_entry;
struct ks_vm_map_store;
typedef struct ks_vm_map_store ks_vm_map_store;
struct ks_vm_shared_region_slide_info;
typedef struct ks_vm_shared_region_slide_info ks_vm_shared_region_slide_info;

enum {
    CLUSTER_TYPE_INVALID = -1,
    CLUSTER_TYPE_SMP = 0,
    CLUSTER_TYPE_E = 1,
    CLUSTER_TYPE_P = 2,
    MAX_CPU_TYPES = 3,
};

enum {
    PRIO_DARWIN_GPU_UNKNOWN = 0,
    PRIO_DARWIN_GPU_ALLOW = 1,
    PRIO_DARWIN_GPU_DENY = 2,
    PRIO_DARWIN_GPU_BACKGROUND = 3,
    PRIO_DARWIN_GPU_UTILITY = 4,
    PRIO_DARWIN_GPU_UI_NON_FOCAL = 5,
    PRIO_DARWIN_GPU_UI = 6,
    PRIO_DARWIN_GPU_UI_FOCAL = 7,
};

enum {
    IO_STATE_INACTIVE = 0,
    IO_STATE_IN_SPACE = 1,
    IO_STATE_IN_SPACE_IMMOVABLE = 2,
    IO_STATE_IN_LIMBO = 3,
    IO_STATE_IN_LIMBO_PD = 4,
    IO_STATE_IN_TRANSIT = 5,
    IO_STATE_IN_TRANSIT_PD = 6,
};

enum {
    IOT_PORT_SET = 0,
    IOT_PORT = 1,
    IOT_SERVICE_PORT = 2,
    IOT_BOOTSTRAP_PORT = 3,
    IOT_WEAK_SERVICE_PORT = 4,
    IOT_CONNECTION_PORT = 5,
    IOT_CONNECTION_PORT_WITH_PORT_ARRAY = 6,
    IOT_EXCEPTION_PORT = 7,
    IOT_TIMER_PORT = 8,
    IOT_REPLY_PORT = 9,
    IOT_SPECIAL_REPLY_PORT = 10,
    IOT_PROVISIONAL_REPLY_PORT = 11,
    __IKOT_FIRST = 12,
    IKOT_THREAD_CONTROL = 12,
    IKOT_THREAD_READ = 13,
    IKOT_THREAD_INSPECT = 14,
    IKOT_TASK_CONTROL = 15,
    IKOT_TASK_READ = 16,
    IKOT_TASK_INSPECT = 17,
    IKOT_TASK_NAME = 18,
    IKOT_TASK_RESUME = 19,
    IKOT_TASK_ID_TOKEN = 20,
    IKOT_TASK_FATAL = 21,
    IKOT_HOST = 22,
    IKOT_HOST_PRIV = 23,
    IKOT_CLOCK = 24,
    IKOT_PROCESSOR = 25,
    IKOT_PROCESSOR_SET = 26,
    IKOT_PROCESSOR_SET_NAME = 27,
    IKOT_EVENTLINK = 28,
    IKOT_FILEPORT = 29,
    IKOT_SEMAPHORE = 30,
    IKOT_VOUCHER = 31,
    IKOT_WORK_INTERVAL = 32,
    IKOT_MEMORY_OBJECT = 33,
    IKOT_NAMED_ENTRY = 34,
    IKOT_MAIN_DEVICE = 35,
    IKOT_IOKIT_IDENT = 36,
    IKOT_IOKIT_CONNECT = 37,
    IKOT_IOKIT_OBJECT = 38,
    IKOT_UEXT_OBJECT = 39,
    IKOT_EXCLAVES_RESOURCE = 40,
    IKOT_ARCADE_REG = 41,
    IKOT_AU_SESSIONPORT = 42,
    IKOT_HYPERVISOR = 43,
    IKOT_KCDATA = 44,
    IKOT_UND_REPLY = 45,
    IKOT_UX_HANDLER = 46,
    IOT_UNKNOWN = 47,
    IOT_ANY = 255,
};

enum {
    IS_HAS_BOOTSTRAP_PORT_TELEMETRY = 1,
    IS_HAS_CREATE_PRP_TELEMETRY = 2,
    IS_HAS_MOVE_PRP_TELEMETRY = 4,
};

enum {
    KCD_CD_FLAG_IN_MARK = 1,
    KCD_CD_FLAG_FINALIZE = 2,
};

enum {
    KCDCT_NONE = 0,
    KCDCT_ZLIB = 1,
};

enum {
    PSET_SMP = 0,
    PSET_AMP_E = 1,
    PSET_AMP_P = 2,
    MAX_PSET_TYPES = 3,
};

enum {
    TH_BUCKET_FIXPRI = 0,
    TH_BUCKET_SHARE_FG = 1,
    TH_BUCKET_SHARE_IN = 2,
    TH_BUCKET_SHARE_DF = 3,
    TH_BUCKET_SHARE_UT = 4,
    TH_BUCKET_SHARE_BG = 5,
    TH_BUCKET_RUN = 6,
    TH_BUCKET_SCHED_MAX = 6,
    TH_BUCKET_MAX = 7,
};

enum {
    TASK_CONTROL_PORT_OPTIONS_INVALID = 0,
    TASK_CONTROL_PORT_OPTIONS_NONE = 1,
    TASK_CONTROL_PORT_IMMOVABLE_HARD = 2,
    TASK_CONTROL_PORT_IMMOVABLE_MASK = 2,
};

enum {
    TASK_MEMLIMIT_IS_ACTIVE = 1,
    TASK_MEMLIMIT_IS_FATAL = 2,
    TASK_MEMLIMIT_ACTIVE_EXC_RESOURCE = 4,
    TASK_MEMLIMIT_INACTIVE_EXC_RESOURCE = 8,
};

enum {
    WQT_INVALID = 0,
    WQT_QUEUE = 1,
    WQT_TURNSTILE = 2,
    WQT_PORT = 3,
    WQT_SELECT = 4,
    WQT_PORT_SET = 5,
    WQT_SELECT_SET = 6,
};

struct ks___smrq_slink_t {
    volatile struct ks_smrq_slink *__smr_ptr;
};
_Static_assert(offsetof(struct ks___smrq_slink_t, __smr_ptr) == 0x0, "ks___smrq_slink_t.__smr_ptr offset");

struct ks__cpu_time_qos_stats {
    unsigned long long cpu_time_qos_default;
    unsigned long long cpu_time_qos_maintenance;
    unsigned long long cpu_time_qos_background;
    unsigned long long cpu_time_qos_utility;
    unsigned long long cpu_time_qos_legacy;
    unsigned long long cpu_time_qos_user_initiated;
    unsigned long long cpu_time_qos_user_interactive;
};
_Static_assert(offsetof(struct ks__cpu_time_qos_stats, cpu_time_qos_default) == 0x0, "ks__cpu_time_qos_stats.cpu_time_qos_default offset");
_Static_assert(offsetof(struct ks__cpu_time_qos_stats, cpu_time_qos_maintenance) == 0x8, "ks__cpu_time_qos_stats.cpu_time_qos_maintenance offset");
_Static_assert(offsetof(struct ks__cpu_time_qos_stats, cpu_time_qos_background) == 0x10, "ks__cpu_time_qos_stats.cpu_time_qos_background offset");
_Static_assert(offsetof(struct ks__cpu_time_qos_stats, cpu_time_qos_utility) == 0x18, "ks__cpu_time_qos_stats.cpu_time_qos_utility offset");
_Static_assert(offsetof(struct ks__cpu_time_qos_stats, cpu_time_qos_legacy) == 0x20, "ks__cpu_time_qos_stats.cpu_time_qos_legacy offset");
_Static_assert(offsetof(struct ks__cpu_time_qos_stats, cpu_time_qos_user_initiated) == 0x28, "ks__cpu_time_qos_stats.cpu_time_qos_user_initiated offset");
_Static_assert(offsetof(struct ks__cpu_time_qos_stats, cpu_time_qos_user_interactive) == 0x30, "ks__cpu_time_qos_stats.cpu_time_qos_user_interactive offset");

struct ks_lck_rw_word_t___anon_member_2 {
    unsigned short shared_count;
    unsigned short interlock : 1; /* bit offset 16 */
    unsigned short priv_excl : 1; /* bit offset 17 */
    unsigned short want_upgrade : 1; /* bit offset 18 */
    unsigned short want_excl : 1; /* bit offset 19 */
    unsigned short r_waiting : 1; /* bit offset 20 */
    unsigned short w_waiting : 1; /* bit offset 21 */
    unsigned short can_sleep : 1; /* bit offset 22 */
    unsigned short _pad2 : 8; /* bit offset 23 */
    unsigned short tag_valid : 1; /* bit offset 31 */
};
_Static_assert(offsetof(struct ks_lck_rw_word_t___anon_member_2, shared_count) == 0x0, "ks_lck_rw_word_t___anon_member_2.shared_count offset");

union ks_lck_rw_word_t {
    struct ks_lck_rw_word_t___anon_member_2 __anon_member_2;
    unsigned int data;
};
_Static_assert(offsetof(union ks_lck_rw_word_t, __anon_member_2) == 0x0, "ks_lck_rw_word_t.__anon_member_2 offset");
_Static_assert(offsetof(union ks_lck_rw_word_t, data) == 0x0, "ks_lck_rw_word_t.data offset");

struct ks_lck_rw_s {
    unsigned int lck_rw_unused : 24; /* bit offset 0 */
    unsigned int lck_rw_type : 8; /* bit offset 24 */
    unsigned int lck_rw_padding;
    union ks_lck_rw_word_t lck_rw;
    unsigned int lck_rw_owner;
};
_Static_assert(offsetof(struct ks_lck_rw_s, lck_rw_padding) == 0x4, "ks_lck_rw_s.lck_rw_padding offset");
_Static_assert(offsetof(struct ks_lck_rw_s, lck_rw) == 0x8, "ks_lck_rw_s.lck_rw offset");
_Static_assert(offsetof(struct ks_lck_rw_s, lck_rw_owner) == 0xc, "ks_lck_rw_s.lck_rw_owner offset");

struct ks_rb_head {
    struct ks_vm_map_store *rbh_root;
};
_Static_assert(offsetof(struct ks_rb_head, rbh_root) == 0x0, "ks_rb_head.rbh_root offset");

struct ks_vm_map_links {
    unsigned long prev : 48; /* bit offset 0 */
    unsigned char vme_zero_wire_count_waiters : 1; /* bit offset 48 */
    struct ks_vm_map_entry *next;
    unsigned long long start;
    unsigned long long end;
};
_Static_assert(offsetof(struct ks_vm_map_links, next) == 0x8, "ks_vm_map_links.next offset");
_Static_assert(offsetof(struct ks_vm_map_links, start) == 0x10, "ks_vm_map_links.start offset");
_Static_assert(offsetof(struct ks_vm_map_links, end) == 0x18, "ks_vm_map_links.end offset");

struct ks_vm_map_header {
    struct ks_vm_map_links links;
    int nentries;
    unsigned short page_shift;
    unsigned short entries_pageable : 1; /* bit offset 304 */
    unsigned short __padding : 15; /* bit offset 305 */
    struct ks_rb_head rb_head_store;
};
_Static_assert(offsetof(struct ks_vm_map_header, links) == 0x0, "ks_vm_map_header.links offset");
_Static_assert(offsetof(struct ks_vm_map_header, nentries) == 0x20, "ks_vm_map_header.nentries offset");
_Static_assert(offsetof(struct ks_vm_map_header, page_shift) == 0x24, "ks_vm_map_header.page_shift offset");
_Static_assert(offsetof(struct ks_vm_map_header, rb_head_store) == 0x28, "ks_vm_map_header.rb_head_store offset");

union ks__vm_map_f_s {
    struct ks_vm_map_entry *_first_free;
    struct ks_vm_map_links *_holes;
};
_Static_assert(offsetof(union ks__vm_map_f_s, _first_free) == 0x0, "ks__vm_map_f_s._first_free offset");
_Static_assert(offsetof(union ks__vm_map_f_s, _holes) == 0x0, "ks__vm_map_f_s._holes offset");

union ks__vm_map_vmmap_u_1 {
    struct ks_vm_map_links *vmmap_hole_hint;
    struct ks_vm_map_corpse_footprint_header *vmmap_corpse_footprint;
};
_Static_assert(offsetof(union ks__vm_map_vmmap_u_1, vmmap_hole_hint) == 0x0, "ks__vm_map_vmmap_u_1.vmmap_hole_hint offset");
_Static_assert(offsetof(union ks__vm_map_vmmap_u_1, vmmap_corpse_footprint) == 0x0, "ks__vm_map_vmmap_u_1.vmmap_corpse_footprint offset");

struct ks__vm_map {
    struct ks_lck_rw_s lock;
    struct ks_vm_map_header hdr;
    struct ks_pmap *pmap;
    unsigned long long size;
    unsigned long long size_limit;
    unsigned long long data_limit;
    unsigned long long user_wire_limit;
    unsigned long long user_wire_size;
    unsigned int map_refcnt;
    unsigned long long vmu1;
    struct ks_vm_map_entry *hint;
    union ks__vm_map_vmmap_u_1 vmmap_u_1;
    union ks__vm_map_f_s f_s;
    unsigned int wait_for_space : 1; /* bit offset 1216 */
    unsigned int wiring_required : 1; /* bit offset 1217 */
    unsigned int no_zero_fill : 1; /* bit offset 1218 */
    unsigned int mapped_in_other_pmaps : 1; /* bit offset 1219 */
    unsigned int switch_protect : 1; /* bit offset 1220 */
    unsigned int disable_vmentry_reuse : 1; /* bit offset 1221 */
    unsigned int map_disallow_data_exec : 1; /* bit offset 1222 */
    unsigned int holelistenabled : 1; /* bit offset 1223 */
    unsigned int is_nested_map : 1; /* bit offset 1224 */
    unsigned int map_disallow_new_exec : 1; /* bit offset 1225 */
    unsigned int jit_entry_exists : 1; /* bit offset 1226 */
    unsigned int has_corpse_footprint : 1; /* bit offset 1227 */
    unsigned int terminated : 1; /* bit offset 1228 */
    unsigned int is_alien : 1; /* bit offset 1229 */
    unsigned int cs_enforcement : 1; /* bit offset 1230 */
    unsigned int cs_debugged : 1; /* bit offset 1231 */
    unsigned int reserved_regions : 1; /* bit offset 1232 */
    unsigned int single_jit : 1; /* bit offset 1233 */
    unsigned int never_faults : 1; /* bit offset 1234 */
    unsigned int uses_user_ranges : 1; /* bit offset 1235 */
    unsigned int tpro_enforcement : 1; /* bit offset 1236 */
    unsigned int corpse_source : 1; /* bit offset 1237 */
    unsigned int cs_platform_binary : 1; /* bit offset 1238 */
    unsigned int vmmap_sealed : 2; /* bit offset 1239 */
    unsigned int res0 : 1; /* bit offset 1241 */
    unsigned int pad : 6; /* bit offset 1242 */
    unsigned long long timestamp;
    struct ks_task *owning_task;
    const void *serial_id;
};
_Static_assert(offsetof(struct ks__vm_map, lock) == 0x0, "ks__vm_map.lock offset");
_Static_assert(offsetof(struct ks__vm_map, hdr) == 0x10, "ks__vm_map.hdr offset");
_Static_assert(offsetof(struct ks__vm_map, pmap) == 0x40, "ks__vm_map.pmap offset");
_Static_assert(offsetof(struct ks__vm_map, size) == 0x48, "ks__vm_map.size offset");
_Static_assert(offsetof(struct ks__vm_map, size_limit) == 0x50, "ks__vm_map.size_limit offset");
_Static_assert(offsetof(struct ks__vm_map, data_limit) == 0x58, "ks__vm_map.data_limit offset");
_Static_assert(offsetof(struct ks__vm_map, user_wire_limit) == 0x60, "ks__vm_map.user_wire_limit offset");
_Static_assert(offsetof(struct ks__vm_map, user_wire_size) == 0x68, "ks__vm_map.user_wire_size offset");
_Static_assert(offsetof(struct ks__vm_map, map_refcnt) == 0x70, "ks__vm_map.map_refcnt offset");
_Static_assert(offsetof(struct ks__vm_map, vmu1) == 0x78, "ks__vm_map.vmu1 offset");
_Static_assert(offsetof(struct ks__vm_map, hint) == 0x80, "ks__vm_map.hint offset");
_Static_assert(offsetof(struct ks__vm_map, vmmap_u_1) == 0x88, "ks__vm_map.vmmap_u_1 offset");
_Static_assert(offsetof(struct ks__vm_map, f_s) == 0x90, "ks__vm_map.f_s offset");
_Static_assert(offsetof(struct ks__vm_map, timestamp) == 0xa0, "ks__vm_map.timestamp offset");
_Static_assert(offsetof(struct ks__vm_map, owning_task) == 0xa8, "ks__vm_map.owning_task offset");
_Static_assert(offsetof(struct ks__vm_map, serial_id) == 0xb0, "ks__vm_map.serial_id offset");

struct ks__vm_object_query_data____anon_member_30 {
    unsigned long long vo_no_footprint : 1; /* bit offset 0 */
    unsigned long long vo_ledger_tag : 3; /* bit offset 1 */
    unsigned long long purgable : 2; /* bit offset 4 */
};

struct ks__vm_object_query_data_ {
    unsigned long long object_id;
    unsigned long long virtual_size;
    unsigned long long resident_size;
    unsigned long long wired_size;
    unsigned long long reusable_size;
    unsigned long long compressed_size;
    struct ks__vm_object_query_data____anon_member_30 __anon_member_30;
};
_Static_assert(offsetof(struct ks__vm_object_query_data_, object_id) == 0x0, "ks__vm_object_query_data_.object_id offset");
_Static_assert(offsetof(struct ks__vm_object_query_data_, virtual_size) == 0x8, "ks__vm_object_query_data_.virtual_size offset");
_Static_assert(offsetof(struct ks__vm_object_query_data_, resident_size) == 0x10, "ks__vm_object_query_data_.resident_size offset");
_Static_assert(offsetof(struct ks__vm_object_query_data_, wired_size) == 0x18, "ks__vm_object_query_data_.wired_size offset");
_Static_assert(offsetof(struct ks__vm_object_query_data_, reusable_size) == 0x20, "ks__vm_object_query_data_.reusable_size offset");
_Static_assert(offsetof(struct ks__vm_object_query_data_, compressed_size) == 0x28, "ks__vm_object_query_data_.compressed_size offset");
_Static_assert(offsetof(struct ks__vm_object_query_data_, __anon_member_30) == 0x30, "ks__vm_object_query_data_.__anon_member_30 offset");

struct ks__vmobject_list_output_ {
    unsigned long long entries;
    struct ks__vm_object_query_data_ data[0];
};
_Static_assert(offsetof(struct ks__vmobject_list_output_, entries) == 0x0, "ks__vmobject_list_output_.entries offset");
_Static_assert(offsetof(struct ks__vmobject_list_output_, data) == 0x8, "ks__vmobject_list_output_.data offset");

struct ks_lck_mtx_state___anon_member_1 {
    unsigned int owner : 28; /* bit offset 0 */
    unsigned int ilocked : 1; /* bit offset 28 */
    unsigned int spin_mode : 1; /* bit offset 29 */
    unsigned int needs_wakeup : 1; /* bit offset 30 */
    unsigned int profile : 1; /* bit offset 31 */
    unsigned short ilk_tail;
    unsigned short as_tail;
};
_Static_assert(offsetof(struct ks_lck_mtx_state___anon_member_1, ilk_tail) == 0x4, "ks_lck_mtx_state___anon_member_1.ilk_tail offset");
_Static_assert(offsetof(struct ks_lck_mtx_state___anon_member_1, as_tail) == 0x6, "ks_lck_mtx_state___anon_member_1.as_tail offset");

union ks_lck_mtx_state {
    struct ks_lck_mtx_state___anon_member_1 __anon_member_1;
    unsigned int data;
    unsigned long long val;
};
_Static_assert(offsetof(union ks_lck_mtx_state, __anon_member_1) == 0x0, "ks_lck_mtx_state.__anon_member_1 offset");
_Static_assert(offsetof(union ks_lck_mtx_state, data) == 0x0, "ks_lck_mtx_state.data offset");
_Static_assert(offsetof(union ks_lck_mtx_state, val) == 0x0, "ks_lck_mtx_state.val offset");

struct ks_lck_mtx_s {
    unsigned int lck_mtx_tsid : 24; /* bit offset 0 */
    unsigned char lck_mtx_type : 8; /* bit offset 24 */
    unsigned int lck_mtx_grp;
    union ks_lck_mtx_state lck_mtx;
};
_Static_assert(offsetof(struct ks_lck_mtx_s, lck_mtx_grp) == 0x4, "ks_lck_mtx_s.lck_mtx_grp offset");
_Static_assert(offsetof(struct ks_lck_mtx_s, lck_mtx) == 0x8, "ks_lck_mtx_s.lck_mtx offset");

struct ks_queue_entry {
    struct ks_queue_entry *next;
    struct ks_queue_entry *prev;
};
_Static_assert(offsetof(struct ks_queue_entry, next) == 0x0, "ks_queue_entry.next offset");
_Static_assert(offsetof(struct ks_queue_entry, prev) == 0x8, "ks_queue_entry.prev offset");

struct ks_affinity_space {
    struct ks_lck_mtx_s aspc_lock;
    unsigned int aspc_task_count;
    struct ks_queue_entry aspc_affinities;
};
_Static_assert(offsetof(struct ks_affinity_space, aspc_lock) == 0x0, "ks_affinity_space.aspc_lock offset");
_Static_assert(offsetof(struct ks_affinity_space, aspc_task_count) == 0x10, "ks_affinity_space.aspc_task_count offset");
_Static_assert(offsetof(struct ks_affinity_space, aspc_affinities) == 0x18, "ks_affinity_space.aspc_affinities offset");

struct ks_arm64_uexc_region_t {
    unsigned long long start;
    unsigned long long end;
    unsigned long long handler;
    unsigned long long refcon;
};
_Static_assert(offsetof(struct ks_arm64_uexc_region_t, start) == 0x0, "ks_arm64_uexc_region_t.start offset");
_Static_assert(offsetof(struct ks_arm64_uexc_region_t, end) == 0x8, "ks_arm64_uexc_region_t.end offset");
_Static_assert(offsetof(struct ks_arm64_uexc_region_t, handler) == 0x10, "ks_arm64_uexc_region_t.handler offset");
_Static_assert(offsetof(struct ks_arm64_uexc_region_t, refcon) == 0x18, "ks_arm64_uexc_region_t.refcon offset");

struct ks_audit_token_t {
    unsigned int val[8];
};
_Static_assert(offsetof(struct ks_audit_token_t, val) == 0x0, "ks_audit_token_t.val offset");

struct ks_bank_element {
    unsigned int be_type : 31; /* bit offset 0 */
    unsigned int be_voucher_ref : 1; /* bit offset 31 */
    unsigned int be_refs;
    unsigned int be_made;
};
_Static_assert(offsetof(struct ks_bank_element, be_refs) == 0x4, "ks_bank_element.be_refs offset");
_Static_assert(offsetof(struct ks_bank_element, be_made) == 0x8, "ks_bank_element.be_made offset");

struct ks_proc_persona_info {
    unsigned long long unique_pid;
    int pid;
    unsigned int flags;
    unsigned int pidversion;
    unsigned int persona_id;
    unsigned int uid;
    unsigned int gid;
    unsigned char macho_uuid[16];
};
_Static_assert(offsetof(struct ks_proc_persona_info, unique_pid) == 0x0, "ks_proc_persona_info.unique_pid offset");
_Static_assert(offsetof(struct ks_proc_persona_info, pid) == 0x8, "ks_proc_persona_info.pid offset");
_Static_assert(offsetof(struct ks_proc_persona_info, flags) == 0xc, "ks_proc_persona_info.flags offset");
_Static_assert(offsetof(struct ks_proc_persona_info, pidversion) == 0x10, "ks_proc_persona_info.pidversion offset");
_Static_assert(offsetof(struct ks_proc_persona_info, persona_id) == 0x14, "ks_proc_persona_info.persona_id offset");
_Static_assert(offsetof(struct ks_proc_persona_info, uid) == 0x18, "ks_proc_persona_info.uid offset");
_Static_assert(offsetof(struct ks_proc_persona_info, gid) == 0x1c, "ks_proc_persona_info.gid offset");
_Static_assert(offsetof(struct ks_proc_persona_info, macho_uuid) == 0x20, "ks_proc_persona_info.macho_uuid offset");

struct ks_bank_task {
    struct ks_bank_element bt_elem;
    struct ks_proc_persona_info bt_proc_persona;
    struct ks_ledger *bt_ledger;
    struct ks_queue_entry bt_accounts_to_pay;
    struct ks_queue_entry bt_accounts_to_charge;
    struct ks_lck_mtx_s bt_acc_to_pay_lock;
    struct ks_lck_mtx_s bt_acc_to_charge_lock;
    unsigned int bt_persona_uid;
    unsigned int bt_hasentitlement : 1; /* bit offset 1120 */
    unsigned long long bt_rsrc_coal_id;
    struct ks_thread_group *bt_thread_group;
};
_Static_assert(offsetof(struct ks_bank_task, bt_elem) == 0x0, "ks_bank_task.bt_elem offset");
_Static_assert(offsetof(struct ks_bank_task, bt_proc_persona) == 0x10, "ks_bank_task.bt_proc_persona offset");
_Static_assert(offsetof(struct ks_bank_task, bt_ledger) == 0x40, "ks_bank_task.bt_ledger offset");
_Static_assert(offsetof(struct ks_bank_task, bt_accounts_to_pay) == 0x48, "ks_bank_task.bt_accounts_to_pay offset");
_Static_assert(offsetof(struct ks_bank_task, bt_accounts_to_charge) == 0x58, "ks_bank_task.bt_accounts_to_charge offset");
_Static_assert(offsetof(struct ks_bank_task, bt_acc_to_pay_lock) == 0x68, "ks_bank_task.bt_acc_to_pay_lock offset");
_Static_assert(offsetof(struct ks_bank_task, bt_acc_to_charge_lock) == 0x78, "ks_bank_task.bt_acc_to_charge_lock offset");
_Static_assert(offsetof(struct ks_bank_task, bt_persona_uid) == 0x88, "ks_bank_task.bt_persona_uid offset");
_Static_assert(offsetof(struct ks_bank_task, bt_rsrc_coal_id) == 0x90, "ks_bank_task.bt_rsrc_coal_id offset");
_Static_assert(offsetof(struct ks_bank_task, bt_thread_group) == 0x98, "ks_bank_task.bt_thread_group offset");

struct ks_hslock {
    unsigned long lock_data;
};
_Static_assert(offsetof(struct ks_hslock, lock_data) == 0x0, "ks_hslock.lock_data offset");

struct ks_lck_spin_s {
    struct ks_hslock hwlock;
    unsigned long type;
};
_Static_assert(offsetof(struct ks_lck_spin_s, hwlock) == 0x0, "ks_lck_spin_s.hwlock offset");
_Static_assert(offsetof(struct ks_lck_spin_s, type) == 0x8, "ks_lck_spin_s.type offset");

struct ks_bool_gen {
    unsigned int seed[4];
    unsigned int state;
    struct ks_lck_spin_s lock;
};
_Static_assert(offsetof(struct ks_bool_gen, seed) == 0x0, "ks_bool_gen.seed offset");
_Static_assert(offsetof(struct ks_bool_gen, state) == 0x10, "ks_bool_gen.state offset");
_Static_assert(offsetof(struct ks_bool_gen, lock) == 0x18, "ks_bool_gen.lock offset");

struct ks_circle_queue_head {
    struct ks_queue_entry *head;
};
_Static_assert(offsetof(struct ks_circle_queue_head, head) == 0x0, "ks_circle_queue_head.head offset");

struct ks_coalition_effective_policy {
    unsigned long long cep_darwinbg : 1; /* bit offset 0 */
    unsigned long long cep_reserved : 63; /* bit offset 1 */
};

struct ks_coalition_requested_policy {
    unsigned long long crp_darwinbg : 1; /* bit offset 0 */
    unsigned long long crp_reserved : 63; /* bit offset 1 */
};

struct ks_i_jetsam_coalition {
    struct ks_task *leader;
    struct ks_queue_entry extensions;
    struct ks_queue_entry services;
    struct ks_queue_entry other;
    struct ks_thread_group *thread_group;
    _Bool swap_enabled;
    struct ks_coalition_requested_policy c_requested_policy;
    struct ks_coalition_effective_policy c_effective_policy;
};
_Static_assert(offsetof(struct ks_i_jetsam_coalition, leader) == 0x0, "ks_i_jetsam_coalition.leader offset");
_Static_assert(offsetof(struct ks_i_jetsam_coalition, extensions) == 0x8, "ks_i_jetsam_coalition.extensions offset");
_Static_assert(offsetof(struct ks_i_jetsam_coalition, services) == 0x18, "ks_i_jetsam_coalition.services offset");
_Static_assert(offsetof(struct ks_i_jetsam_coalition, other) == 0x28, "ks_i_jetsam_coalition.other offset");
_Static_assert(offsetof(struct ks_i_jetsam_coalition, thread_group) == 0x38, "ks_i_jetsam_coalition.thread_group offset");
_Static_assert(offsetof(struct ks_i_jetsam_coalition, swap_enabled) == 0x40, "ks_i_jetsam_coalition.swap_enabled offset");
_Static_assert(offsetof(struct ks_i_jetsam_coalition, c_requested_policy) == 0x48, "ks_i_jetsam_coalition.c_requested_policy offset");
_Static_assert(offsetof(struct ks_i_jetsam_coalition, c_effective_policy) == 0x50, "ks_i_jetsam_coalition.c_effective_policy offset");

struct ks_recount_coalition {
    struct ks_recount_usage *rco_exited;
};
_Static_assert(offsetof(struct ks_recount_coalition, rco_exited) == 0x0, "ks_recount_coalition.rco_exited offset");

struct ks_i_resource_coalition {
    struct ks_ledger *ledger;
    unsigned long long bytesread;
    unsigned long long byteswritten;
    unsigned long long energy;
    unsigned long long gpu_time;
    unsigned long long logical_immediate_writes;
    unsigned long long logical_deferred_writes;
    unsigned long long logical_invalidated_writes;
    unsigned long long logical_metadata_writes;
    unsigned long long logical_immediate_writes_to_external;
    unsigned long long logical_deferred_writes_to_external;
    unsigned long long logical_invalidated_writes_to_external;
    unsigned long long logical_metadata_writes_to_external;
    unsigned long long cpu_time_eqos[7];
    unsigned long long cpu_time_rqos[7];
    unsigned long long cpu_instructions;
    unsigned long long cpu_cycles;
    unsigned long long ane_mach_time;
    unsigned long long ane_energy_nj;
    unsigned long long gpu_energy_nj;
    unsigned long long gpu_energy_nj_billed_to_me;
    unsigned long long gpu_energy_nj_billed_to_others;
    struct ks_recount_coalition co_recount;
    unsigned long long task_count;
    unsigned long long dead_task_count;
    unsigned long long last_became_nonempty_time;
    unsigned long long time_nonempty;
    struct ks_queue_entry tasks;
    struct ks_ledger *resource_monitor_ledger;
    unsigned long long fs_metadata_writes;
};
_Static_assert(offsetof(struct ks_i_resource_coalition, ledger) == 0x0, "ks_i_resource_coalition.ledger offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, bytesread) == 0x8, "ks_i_resource_coalition.bytesread offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, byteswritten) == 0x10, "ks_i_resource_coalition.byteswritten offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, energy) == 0x18, "ks_i_resource_coalition.energy offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, gpu_time) == 0x20, "ks_i_resource_coalition.gpu_time offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_immediate_writes) == 0x28, "ks_i_resource_coalition.logical_immediate_writes offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_deferred_writes) == 0x30, "ks_i_resource_coalition.logical_deferred_writes offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_invalidated_writes) == 0x38, "ks_i_resource_coalition.logical_invalidated_writes offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_metadata_writes) == 0x40, "ks_i_resource_coalition.logical_metadata_writes offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_immediate_writes_to_external) == 0x48, "ks_i_resource_coalition.logical_immediate_writes_to_external offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_deferred_writes_to_external) == 0x50, "ks_i_resource_coalition.logical_deferred_writes_to_external offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_invalidated_writes_to_external) == 0x58, "ks_i_resource_coalition.logical_invalidated_writes_to_external offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, logical_metadata_writes_to_external) == 0x60, "ks_i_resource_coalition.logical_metadata_writes_to_external offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, cpu_time_eqos) == 0x68, "ks_i_resource_coalition.cpu_time_eqos offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, cpu_time_rqos) == 0xa0, "ks_i_resource_coalition.cpu_time_rqos offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, cpu_instructions) == 0xd8, "ks_i_resource_coalition.cpu_instructions offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, cpu_cycles) == 0xe0, "ks_i_resource_coalition.cpu_cycles offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, ane_mach_time) == 0xe8, "ks_i_resource_coalition.ane_mach_time offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, ane_energy_nj) == 0xf0, "ks_i_resource_coalition.ane_energy_nj offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, gpu_energy_nj) == 0xf8, "ks_i_resource_coalition.gpu_energy_nj offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, gpu_energy_nj_billed_to_me) == 0x100, "ks_i_resource_coalition.gpu_energy_nj_billed_to_me offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, gpu_energy_nj_billed_to_others) == 0x108, "ks_i_resource_coalition.gpu_energy_nj_billed_to_others offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, co_recount) == 0x110, "ks_i_resource_coalition.co_recount offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, task_count) == 0x118, "ks_i_resource_coalition.task_count offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, dead_task_count) == 0x120, "ks_i_resource_coalition.dead_task_count offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, last_became_nonempty_time) == 0x128, "ks_i_resource_coalition.last_became_nonempty_time offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, time_nonempty) == 0x130, "ks_i_resource_coalition.time_nonempty offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, tasks) == 0x138, "ks_i_resource_coalition.tasks offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, resource_monitor_ledger) == 0x148, "ks_i_resource_coalition.resource_monitor_ledger offset");
_Static_assert(offsetof(struct ks_i_resource_coalition, fs_metadata_writes) == 0x150, "ks_i_resource_coalition.fs_metadata_writes offset");

struct ks_smr_node {
    struct ks_smr_node *smrn_next;
    void *smrn_cb;
};
_Static_assert(offsetof(struct ks_smr_node, smrn_next) == 0x0, "ks_smr_node.smrn_next offset");
_Static_assert(offsetof(struct ks_smr_node, smrn_cb) == 0x8, "ks_smr_node.smrn_cb offset");

struct ks_smrq_slink {
    struct ks___smrq_slink_t next;
};
_Static_assert(offsetof(struct ks_smrq_slink, next) == 0x0, "ks_smrq_slink.next offset");

struct ks_coalition {
    unsigned long long id;
    unsigned int type;
    unsigned int role;
    unsigned int ref_count;
    unsigned int active_count;
    unsigned int focal_task_count;
    unsigned int nonfocal_task_count;
    unsigned int game_task_count;
    unsigned int carplay_task_count;
    unsigned int privileged : 1; /* bit offset 320 */
    unsigned int termrequested : 1; /* bit offset 321 */
    unsigned int terminated : 1; /* bit offset 322 */
    unsigned int reaped : 1; /* bit offset 323 */
    unsigned int notified : 1; /* bit offset 324 */
    unsigned int efficient : 1; /* bit offset 325 */
    struct ks_smrq_slink link;
    union {
        struct ks_lck_mtx_s lock;
        struct ks_smr_node smr_node;
    };
    union {
        struct ks_i_resource_coalition r;
        struct ks_i_jetsam_coalition j;
    };
};
_Static_assert(offsetof(struct ks_coalition, id) == 0x0, "ks_coalition.id offset");
_Static_assert(offsetof(struct ks_coalition, type) == 0x8, "ks_coalition.type offset");
_Static_assert(offsetof(struct ks_coalition, role) == 0xc, "ks_coalition.role offset");
_Static_assert(offsetof(struct ks_coalition, ref_count) == 0x10, "ks_coalition.ref_count offset");
_Static_assert(offsetof(struct ks_coalition, active_count) == 0x14, "ks_coalition.active_count offset");
_Static_assert(offsetof(struct ks_coalition, focal_task_count) == 0x18, "ks_coalition.focal_task_count offset");
_Static_assert(offsetof(struct ks_coalition, nonfocal_task_count) == 0x1c, "ks_coalition.nonfocal_task_count offset");
_Static_assert(offsetof(struct ks_coalition, game_task_count) == 0x20, "ks_coalition.game_task_count offset");
_Static_assert(offsetof(struct ks_coalition, carplay_task_count) == 0x24, "ks_coalition.carplay_task_count offset");
_Static_assert(offsetof(struct ks_coalition, link) == 0x30, "ks_coalition.link offset");
_Static_assert(offsetof(struct ks_coalition, lock) == 0x38, "ks_coalition.lock offset");
_Static_assert(offsetof(struct ks_coalition, smr_node) == 0x38, "ks_coalition.smr_node offset");
_Static_assert(offsetof(struct ks_coalition, r) == 0x48, "ks_coalition.r offset");
_Static_assert(offsetof(struct ks_coalition, j) == 0x48, "ks_coalition.j offset");

struct ks_exception_action {
    struct ks_ipc_port *port;
    int flavor;
    int behavior;
    int privileged;
    int hardened;
    struct ks_label *label;
};
_Static_assert(offsetof(struct ks_exception_action, port) == 0x0, "ks_exception_action.port offset");
_Static_assert(offsetof(struct ks_exception_action, flavor) == 0x8, "ks_exception_action.flavor offset");
_Static_assert(offsetof(struct ks_exception_action, behavior) == 0xc, "ks_exception_action.behavior offset");
_Static_assert(offsetof(struct ks_exception_action, privileged) == 0x10, "ks_exception_action.privileged offset");
_Static_assert(offsetof(struct ks_exception_action, hardened) == 0x14, "ks_exception_action.hardened offset");
_Static_assert(offsetof(struct ks_exception_action, label) == 0x18, "ks_exception_action.label offset");

struct ks_gate___anon_member_31___anon_member_32 {
    unsigned int gt_refs : 16; /* bit offset 0 */
    unsigned int gt_alloc : 1; /* bit offset 16 */
    unsigned int gt_type : 2; /* bit offset 17 */
    unsigned int gt_flags_pad : 13; /* bit offset 19 */
};

struct ks_gate {
    unsigned long gt_data;
    struct ks_turnstile *gt_turnstile;
    union {
        struct ks_gate___anon_member_31___anon_member_32 __anon_member_32;
        unsigned int gt_flags;
    };
};
_Static_assert(offsetof(struct ks_gate, gt_data) == 0x0, "ks_gate.gt_data offset");
_Static_assert(offsetof(struct ks_gate, gt_turnstile) == 0x8, "ks_gate.gt_turnstile offset");
_Static_assert(offsetof(struct ks_gate, __anon_member_32) == 0x10, "ks_gate.__anon_member_32 offset");
_Static_assert(offsetof(struct ks_gate, gt_flags) == 0x10, "ks_gate.gt_flags offset");

struct ks_hardened_exception_action {
    struct ks_exception_action ea;
    unsigned int signed_pc_key;
    unsigned int exception;
};
_Static_assert(offsetof(struct ks_hardened_exception_action, ea) == 0x0, "ks_hardened_exception_action.ea offset");
_Static_assert(offsetof(struct ks_hardened_exception_action, signed_pc_key) == 0x20, "ks_hardened_exception_action.signed_pc_key offset");
_Static_assert(offsetof(struct ks_hardened_exception_action, exception) == 0x24, "ks_hardened_exception_action.exception offset");

struct ks_hw_lck_ticket_s___anon_member_3___anon_member_4___anon_member_5 {
    unsigned char cticket;
    unsigned char nticket;
};
_Static_assert(offsetof(struct ks_hw_lck_ticket_s___anon_member_3___anon_member_4___anon_member_5, cticket) == 0x0, "ks_hw_lck_ticket_s___anon_member_3___anon_member_4___anon_member_5.cticket offset");
_Static_assert(offsetof(struct ks_hw_lck_ticket_s___anon_member_3___anon_member_4___anon_member_5, nticket) == 0x1, "ks_hw_lck_ticket_s___anon_member_3___anon_member_4___anon_member_5.nticket offset");

struct ks_hw_lck_ticket_s___anon_member_3 {
    unsigned char lck_type;
    unsigned char lck_valid : 1; /* bit offset 8 */
    unsigned char lck_is_pv : 1; /* bit offset 9 */
    unsigned char lck_unused : 6; /* bit offset 10 */
    union {
        struct ks_hw_lck_ticket_s___anon_member_3___anon_member_4___anon_member_5 __anon_member_5;
        unsigned short tcurnext;
    };
};
_Static_assert(offsetof(struct ks_hw_lck_ticket_s___anon_member_3, lck_type) == 0x0, "ks_hw_lck_ticket_s___anon_member_3.lck_type offset");
_Static_assert(offsetof(struct ks_hw_lck_ticket_s___anon_member_3, __anon_member_5) == 0x2, "ks_hw_lck_ticket_s___anon_member_3.__anon_member_5 offset");
_Static_assert(offsetof(struct ks_hw_lck_ticket_s___anon_member_3, tcurnext) == 0x2, "ks_hw_lck_ticket_s___anon_member_3.tcurnext offset");

struct ks_io_stat_entry {
    unsigned long long count;
    unsigned long long size;
};
_Static_assert(offsetof(struct ks_io_stat_entry, count) == 0x0, "ks_io_stat_entry.count offset");
_Static_assert(offsetof(struct ks_io_stat_entry, size) == 0x8, "ks_io_stat_entry.size offset");

struct ks_io_stat_info {
    struct ks_io_stat_entry disk_reads;
    struct ks_io_stat_entry io_priority[4];
    struct ks_io_stat_entry paging;
    struct ks_io_stat_entry metadata;
    struct ks_io_stat_entry total_io;
};
_Static_assert(offsetof(struct ks_io_stat_info, disk_reads) == 0x0, "ks_io_stat_info.disk_reads offset");
_Static_assert(offsetof(struct ks_io_stat_info, io_priority) == 0x10, "ks_io_stat_info.io_priority offset");
_Static_assert(offsetof(struct ks_io_stat_info, paging) == 0x50, "ks_io_stat_info.paging offset");
_Static_assert(offsetof(struct ks_io_stat_info, metadata) == 0x60, "ks_io_stat_info.metadata offset");
_Static_assert(offsetof(struct ks_io_stat_info, total_io) == 0x70, "ks_io_stat_info.total_io offset");

struct ks_ipc_importance_elem {
    unsigned int iie_bits;
    unsigned int iie_made;
    struct ks_queue_entry iie_kmsgs;
    unsigned int iie_externcnt;
    unsigned int iie_externdrop;
};
_Static_assert(offsetof(struct ks_ipc_importance_elem, iie_bits) == 0x0, "ks_ipc_importance_elem.iie_bits offset");
_Static_assert(offsetof(struct ks_ipc_importance_elem, iie_made) == 0x4, "ks_ipc_importance_elem.iie_made offset");
_Static_assert(offsetof(struct ks_ipc_importance_elem, iie_kmsgs) == 0x8, "ks_ipc_importance_elem.iie_kmsgs offset");
_Static_assert(offsetof(struct ks_ipc_importance_elem, iie_externcnt) == 0x18, "ks_ipc_importance_elem.iie_externcnt offset");
_Static_assert(offsetof(struct ks_ipc_importance_elem, iie_externdrop) == 0x1c, "ks_ipc_importance_elem.iie_externdrop offset");

struct ks_ipc_importance_task {
    struct ks_ipc_importance_elem iit_elem;
    struct ks_task *iit_task;
    struct ks_queue_entry iit_inherits;
    struct ks_queue_entry *iit_updateq;
    struct ks_queue_entry iit_updates;
    struct ks_queue_entry iit_props;
    unsigned long long iit_updatetime;
    unsigned long long iit_transitions;
    unsigned int iit_assertcnt;
    unsigned int iit_legacy_externcnt;
    unsigned int iit_legacy_externdrop;
    unsigned int iit_receiver : 1; /* bit offset 992 */
    unsigned int iit_denap : 1; /* bit offset 993 */
    unsigned int iit_donor : 1; /* bit offset 994 */
    unsigned int iit_live_donor : 1; /* bit offset 995 */
    unsigned int iit_updatepolicy : 1; /* bit offset 996 */
    unsigned int iit_reserved : 3; /* bit offset 997 */
    unsigned int iit_filelocks : 24; /* bit offset 1000 */
};
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_elem) == 0x0, "ks_ipc_importance_task.iit_elem offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_task) == 0x20, "ks_ipc_importance_task.iit_task offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_inherits) == 0x28, "ks_ipc_importance_task.iit_inherits offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_updateq) == 0x38, "ks_ipc_importance_task.iit_updateq offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_updates) == 0x40, "ks_ipc_importance_task.iit_updates offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_props) == 0x50, "ks_ipc_importance_task.iit_props offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_updatetime) == 0x60, "ks_ipc_importance_task.iit_updatetime offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_transitions) == 0x68, "ks_ipc_importance_task.iit_transitions offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_assertcnt) == 0x70, "ks_ipc_importance_task.iit_assertcnt offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_legacy_externcnt) == 0x74, "ks_ipc_importance_task.iit_legacy_externcnt offset");
_Static_assert(offsetof(struct ks_ipc_importance_task, iit_legacy_externdrop) == 0x78, "ks_ipc_importance_task.iit_legacy_externdrop offset");

struct ks_klist {
    struct ks_knote *slh_first;
};
_Static_assert(offsetof(struct ks_klist, slh_first) == 0x0, "ks_klist.slh_first offset");

struct ks_ipc_mqueue {
    struct ks_circle_queue_head imq_messages;
    unsigned int imq_seqno;
    unsigned int imq_receiver_name;
    unsigned short imq_msgcount;
    unsigned short imq_qlimit;
    unsigned int imq_context;
    union {
        struct ks_klist imq_klist;
        struct ks_knote *imq_inheritor_knote;
        struct ks_turnstile *imq_inheritor_turnstile;
        struct ks_thread *imq_inheritor_thread_ref;
        struct ks_thread *imq_srp_owner_thread;
    };
};
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_messages) == 0x0, "ks_ipc_mqueue.imq_messages offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_seqno) == 0x8, "ks_ipc_mqueue.imq_seqno offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_receiver_name) == 0xc, "ks_ipc_mqueue.imq_receiver_name offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_msgcount) == 0x10, "ks_ipc_mqueue.imq_msgcount offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_qlimit) == 0x12, "ks_ipc_mqueue.imq_qlimit offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_context) == 0x14, "ks_ipc_mqueue.imq_context offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_klist) == 0x18, "ks_ipc_mqueue.imq_klist offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_inheritor_knote) == 0x18, "ks_ipc_mqueue.imq_inheritor_knote offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_inheritor_turnstile) == 0x18, "ks_ipc_mqueue.imq_inheritor_turnstile offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_inheritor_thread_ref) == 0x18, "ks_ipc_mqueue.imq_inheritor_thread_ref offset");
_Static_assert(offsetof(struct ks_ipc_mqueue, imq_srp_owner_thread) == 0x18, "ks_ipc_mqueue.imq_srp_owner_thread offset");

struct ks_ipc_object___anon_member_9___anon_member_10 {
    unsigned char io_type;
    unsigned char io_state : 3; /* bit offset 8 */
    unsigned char io_filtered : 1; /* bit offset 11 */
    unsigned char __io_unused1 : 4; /* bit offset 12 */
    _Bool io_label_lock;
    unsigned char __io_unused2;
};
_Static_assert(offsetof(struct ks_ipc_object___anon_member_9___anon_member_10, io_type) == 0x0, "ks_ipc_object___anon_member_9___anon_member_10.io_type offset");
_Static_assert(offsetof(struct ks_ipc_object___anon_member_9___anon_member_10, io_label_lock) == 0x2, "ks_ipc_object___anon_member_9___anon_member_10.io_label_lock offset");
_Static_assert(offsetof(struct ks_ipc_object___anon_member_9___anon_member_10, __io_unused2) == 0x3, "ks_ipc_object___anon_member_9___anon_member_10.__io_unused2 offset");

struct ks_ipc_object {
    union {
        struct ks_ipc_object___anon_member_9___anon_member_10 __anon_member_10;
        unsigned int io_bits;
    };
    unsigned int io_references;
    union {
        const void *iol_pointer;
        unsigned long iol_value;
        struct ks_ipc_service_port_label *iol_service;
        struct ks_ipc_conn_port_label *iol_connection;
        struct ks_ipc_kobject_label *iol_kobject;
        struct ks_mk_timer *iol_mktimer;
    };
};
_Static_assert(offsetof(struct ks_ipc_object, io_references) == 0x4, "ks_ipc_object.io_references offset");
_Static_assert(offsetof(struct ks_ipc_object, __anon_member_10) == 0x0, "ks_ipc_object.__anon_member_10 offset");
_Static_assert(offsetof(struct ks_ipc_object, io_bits) == 0x0, "ks_ipc_object.io_bits offset");
_Static_assert(offsetof(struct ks_ipc_object, iol_pointer) == 0x8, "ks_ipc_object.iol_pointer offset");
_Static_assert(offsetof(struct ks_ipc_object, iol_value) == 0x8, "ks_ipc_object.iol_value offset");
_Static_assert(offsetof(struct ks_ipc_object, iol_service) == 0x8, "ks_ipc_object.iol_service offset");
_Static_assert(offsetof(struct ks_ipc_object, iol_connection) == 0x8, "ks_ipc_object.iol_connection offset");
_Static_assert(offsetof(struct ks_ipc_object, iol_kobject) == 0x8, "ks_ipc_object.iol_kobject offset");
_Static_assert(offsetof(struct ks_ipc_object, iol_mktimer) == 0x8, "ks_ipc_object.iol_mktimer offset");

struct ks_ipc_port___anon_member_12___anon_member_13 {
    unsigned int ip_waitq_type : 3; /* bit offset 0 */
    unsigned int ip_waitq_fifo : 1; /* bit offset 3 */
    unsigned int ip_waitq_preposted : 1; /* bit offset 4 */
    unsigned int ip_fullwaiters : 1; /* bit offset 5 */
    unsigned int ip_sprequests : 1; /* bit offset 6 */
    unsigned int ip_spimportant : 1; /* bit offset 7 */
    unsigned int ip_impdonation : 1; /* bit offset 8 */
    unsigned int ip_tempowner : 1; /* bit offset 9 */
    unsigned int ip_guarded : 1; /* bit offset 10 */
    unsigned int ip_strict_guard : 1; /* bit offset 11 */
    unsigned int ip_sync_link_state : 3; /* bit offset 12 */
    unsigned int ip_sync_bootstrap_checkin : 1; /* bit offset 15 */
    unsigned int ip_tg_block_tracking : 1; /* bit offset 16 */
    unsigned int ip_has_watchport : 1; /* bit offset 17 */
    unsigned int ip_kernel_iotier_override : 2; /* bit offset 18 */
    unsigned int ip_kernel_qos_override : 3; /* bit offset 20 */
    unsigned int ip_srp_lost_link : 1; /* bit offset 23 */
    unsigned int ip_srp_msg_sent : 1; /* bit offset 24 */
    unsigned int __ip_unused : 7; /* bit offset 25 */
};

struct ks_mpsc_queue_chain {
    struct ks_mpsc_queue_chain * _Atomic mpqc_next;
};
_Static_assert(offsetof(struct ks_mpsc_queue_chain, mpqc_next) == 0x0, "ks_mpsc_queue_chain.mpqc_next offset");

struct ks_priority_queue_sched_max {
    struct ks_priority_queue_entry_sched *pq_root;
};
_Static_assert(offsetof(struct ks_priority_queue_sched_max, pq_root) == 0x0, "ks_priority_queue_sched_max.pq_root offset");

struct ks_waitq___anon_member_14 {
    unsigned int waitq_type : 3; /* bit offset 0 */
    unsigned int waitq_fifo : 1; /* bit offset 3 */
    unsigned int waitq_preposted : 1; /* bit offset 4 */
    unsigned int waitq_eventmask : 27; /* bit offset 5 */
};

struct ks_waitq_link_list_entry {
    struct ks_waitq_link_list_entry *next;
};
_Static_assert(offsetof(struct ks_waitq_link_list_entry, next) == 0x0, "ks_waitq_link_list_entry.next offset");

union ks_hw_lck_ticket_s {
    struct ks_hw_lck_ticket_s___anon_member_3 __anon_member_3;
    unsigned int lck_value;
};
_Static_assert(offsetof(union ks_hw_lck_ticket_s, __anon_member_3) == 0x0, "ks_hw_lck_ticket_s.__anon_member_3 offset");
_Static_assert(offsetof(union ks_hw_lck_ticket_s, lck_value) == 0x0, "ks_hw_lck_ticket_s.lck_value offset");

struct ks_waitq {
    struct ks_waitq___anon_member_14 __anon_member_14;
    union ks_hw_lck_ticket_s waitq_interlock;
    unsigned char waitq_padding[0];
    union {
        struct ks_circle_queue_head waitq_queue;
        struct ks_priority_queue_sched_max waitq_prio_queue;
        struct ks_turnstile *waitq_ts;
    };
    union {
        struct ks_circle_queue_head waitq_links;
        struct ks_waitq_link_list_entry waitq_sellinks;
        void *waitq_inheritor;
        struct ks_mpsc_queue_chain waitq_defer;
    };
};
_Static_assert(offsetof(struct ks_waitq, __anon_member_14) == 0x0, "ks_waitq.__anon_member_14 offset");
_Static_assert(offsetof(struct ks_waitq, waitq_interlock) == 0x4, "ks_waitq.waitq_interlock offset");
_Static_assert(offsetof(struct ks_waitq, waitq_padding) == 0x8, "ks_waitq.waitq_padding offset");
_Static_assert(offsetof(struct ks_waitq, waitq_queue) == 0x8, "ks_waitq.waitq_queue offset");
_Static_assert(offsetof(struct ks_waitq, waitq_prio_queue) == 0x8, "ks_waitq.waitq_prio_queue offset");
_Static_assert(offsetof(struct ks_waitq, waitq_ts) == 0x8, "ks_waitq.waitq_ts offset");
_Static_assert(offsetof(struct ks_waitq, waitq_links) == 0x10, "ks_waitq.waitq_links offset");
_Static_assert(offsetof(struct ks_waitq, waitq_sellinks) == 0x10, "ks_waitq.waitq_sellinks offset");
_Static_assert(offsetof(struct ks_waitq, waitq_inheritor) == 0x10, "ks_waitq.waitq_inheritor offset");
_Static_assert(offsetof(struct ks_waitq, waitq_defer) == 0x10, "ks_waitq.waitq_defer offset");

struct ks_ipc_port {
    struct ks_ipc_object ip_object;
    union {
        struct ks_ipc_port___anon_member_12___anon_member_13 __anon_member_13;
        struct ks_waitq ip_waitq;
    };
    struct ks_ipc_mqueue ip_messages;
    union {
        struct ks_ipc_space *ip_receiver;
        struct ks_ipc_port *ip_destination;
        unsigned long ip_timestamp;
    };
    union {
        unsigned long ip_kobject;
        struct ks_ipc_port *ip_nsrequest;
    };
    union {
        struct ks_ipc_importance_task *ip_imp_task;
        struct ks_ipc_port *ip_sync_inheritor_port;
        struct ks_knote *ip_sync_inheritor_knote;
        struct ks_turnstile *ip_sync_inheritor_ts;
    };
    union {
        int ip_pid;
        struct ks_task_watchport_elem *ip_twe;
        struct ks_ipc_port *ip_pdrequest;
    };
    struct ks_ipc_port_request_table *ip_requests;
    struct ks_turnstile *ip_send_turnstile;
    unsigned long long ip_context;
    unsigned int ip_impcount;
    unsigned int ip_mscount;
    unsigned int ip_srights;
    unsigned int ip_sorights;
};
_Static_assert(offsetof(struct ks_ipc_port, ip_object) == 0x0, "ks_ipc_port.ip_object offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_messages) == 0x28, "ks_ipc_port.ip_messages offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_requests) == 0x68, "ks_ipc_port.ip_requests offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_send_turnstile) == 0x70, "ks_ipc_port.ip_send_turnstile offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_context) == 0x78, "ks_ipc_port.ip_context offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_impcount) == 0x80, "ks_ipc_port.ip_impcount offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_mscount) == 0x84, "ks_ipc_port.ip_mscount offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_srights) == 0x88, "ks_ipc_port.ip_srights offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_sorights) == 0x8c, "ks_ipc_port.ip_sorights offset");
_Static_assert(offsetof(struct ks_ipc_port, __anon_member_13) == 0x10, "ks_ipc_port.__anon_member_13 offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_waitq) == 0x10, "ks_ipc_port.ip_waitq offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_receiver) == 0x48, "ks_ipc_port.ip_receiver offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_destination) == 0x48, "ks_ipc_port.ip_destination offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_timestamp) == 0x48, "ks_ipc_port.ip_timestamp offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_kobject) == 0x50, "ks_ipc_port.ip_kobject offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_nsrequest) == 0x50, "ks_ipc_port.ip_nsrequest offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_imp_task) == 0x58, "ks_ipc_port.ip_imp_task offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_sync_inheritor_port) == 0x58, "ks_ipc_port.ip_sync_inheritor_port offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_sync_inheritor_knote) == 0x58, "ks_ipc_port.ip_sync_inheritor_knote offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_sync_inheritor_ts) == 0x58, "ks_ipc_port.ip_sync_inheritor_ts offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_pid) == 0x60, "ks_ipc_port.ip_pid offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_twe) == 0x60, "ks_ipc_port.ip_twe offset");
_Static_assert(offsetof(struct ks_ipc_port, ip_pdrequest) == 0x60, "ks_ipc_port.ip_pdrequest offset");

struct ks_ipc_space_is_table {
    struct ks_ipc_entry_table *__smr_ptr;
};
_Static_assert(offsetof(struct ks_ipc_space_is_table, __smr_ptr) == 0x0, "ks_ipc_space_is_table.__smr_ptr offset");

struct ks_lck_ticket_s {
    unsigned int __lck_ticket_unused : 24; /* bit offset 0 */
    unsigned int lck_ticket_type : 8; /* bit offset 24 */
    unsigned int lck_ticket_padding;
    union ks_hw_lck_ticket_s tu;
    unsigned int lck_ticket_owner;
};
_Static_assert(offsetof(struct ks_lck_ticket_s, lck_ticket_padding) == 0x4, "ks_lck_ticket_s.lck_ticket_padding offset");
_Static_assert(offsetof(struct ks_lck_ticket_s, tu) == 0x8, "ks_lck_ticket_s.tu offset");
_Static_assert(offsetof(struct ks_lck_ticket_s, lck_ticket_owner) == 0xc, "ks_lck_ticket_s.lck_ticket_owner offset");

struct ks_ipc_space {
    struct ks_lck_ticket_s is_lock;
    unsigned int is_bits;
    unsigned int is_table_hashed;
    unsigned int is_table_free;
    unsigned int is_entropy[1];
    struct ks_bool_gen is_prng;
    struct ks_ipc_space_is_table is_table;
    struct ks_task *is_task;
    unsigned long is_policy;
    struct ks_thread *is_grower;
    unsigned long long is_label;
    unsigned int is_low_mod;
    unsigned int is_high_mod;
    unsigned char is_telemetry;
};
_Static_assert(offsetof(struct ks_ipc_space, is_lock) == 0x0, "ks_ipc_space.is_lock offset");
_Static_assert(offsetof(struct ks_ipc_space, is_bits) == 0x10, "ks_ipc_space.is_bits offset");
_Static_assert(offsetof(struct ks_ipc_space, is_table_hashed) == 0x14, "ks_ipc_space.is_table_hashed offset");
_Static_assert(offsetof(struct ks_ipc_space, is_table_free) == 0x18, "ks_ipc_space.is_table_free offset");
_Static_assert(offsetof(struct ks_ipc_space, is_entropy) == 0x1c, "ks_ipc_space.is_entropy offset");
_Static_assert(offsetof(struct ks_ipc_space, is_prng) == 0x20, "ks_ipc_space.is_prng offset");
_Static_assert(offsetof(struct ks_ipc_space, is_table) == 0x48, "ks_ipc_space.is_table offset");
_Static_assert(offsetof(struct ks_ipc_space, is_task) == 0x50, "ks_ipc_space.is_task offset");
_Static_assert(offsetof(struct ks_ipc_space, is_policy) == 0x58, "ks_ipc_space.is_policy offset");
_Static_assert(offsetof(struct ks_ipc_space, is_grower) == 0x60, "ks_ipc_space.is_grower offset");
_Static_assert(offsetof(struct ks_ipc_space, is_label) == 0x68, "ks_ipc_space.is_label offset");
_Static_assert(offsetof(struct ks_ipc_space, is_low_mod) == 0x70, "ks_ipc_space.is_low_mod offset");
_Static_assert(offsetof(struct ks_ipc_space, is_high_mod) == 0x74, "ks_ipc_space.is_high_mod offset");
_Static_assert(offsetof(struct ks_ipc_space, is_telemetry) == 0x78, "ks_ipc_space.is_telemetry offset");

struct ks_z_stream_s {
    unsigned char *next_in;
    unsigned int avail_in;
    unsigned long total_in;
    unsigned char *next_out;
    unsigned int avail_out;
    unsigned long total_out;
    char *msg;
    struct ks_internal_state *state;
    void **zalloc;
    void *zfree;
    void *opaque;
    int data_type;
    unsigned long adler;
    unsigned long reserved;
};
_Static_assert(offsetof(struct ks_z_stream_s, next_in) == 0x0, "ks_z_stream_s.next_in offset");
_Static_assert(offsetof(struct ks_z_stream_s, avail_in) == 0x8, "ks_z_stream_s.avail_in offset");
_Static_assert(offsetof(struct ks_z_stream_s, total_in) == 0x10, "ks_z_stream_s.total_in offset");
_Static_assert(offsetof(struct ks_z_stream_s, next_out) == 0x18, "ks_z_stream_s.next_out offset");
_Static_assert(offsetof(struct ks_z_stream_s, avail_out) == 0x20, "ks_z_stream_s.avail_out offset");
_Static_assert(offsetof(struct ks_z_stream_s, total_out) == 0x28, "ks_z_stream_s.total_out offset");
_Static_assert(offsetof(struct ks_z_stream_s, msg) == 0x30, "ks_z_stream_s.msg offset");
_Static_assert(offsetof(struct ks_z_stream_s, state) == 0x38, "ks_z_stream_s.state offset");
_Static_assert(offsetof(struct ks_z_stream_s, zalloc) == 0x40, "ks_z_stream_s.zalloc offset");
_Static_assert(offsetof(struct ks_z_stream_s, zfree) == 0x48, "ks_z_stream_s.zfree offset");
_Static_assert(offsetof(struct ks_z_stream_s, opaque) == 0x50, "ks_z_stream_s.opaque offset");
_Static_assert(offsetof(struct ks_z_stream_s, data_type) == 0x58, "ks_z_stream_s.data_type offset");
_Static_assert(offsetof(struct ks_z_stream_s, adler) == 0x60, "ks_z_stream_s.adler offset");
_Static_assert(offsetof(struct ks_z_stream_s, reserved) == 0x68, "ks_z_stream_s.reserved offset");

struct ks_kcdata_compress_descriptor {
    struct ks_z_stream_s kcd_cd_zs;
    void *kcd_cd_base;
    unsigned long long kcd_cd_offset;
    unsigned long kcd_cd_maxoffset;
    unsigned long long kcd_cd_mark_begin;
    unsigned long long kcd_cd_flags;
    unsigned long long kcd_cd_compression_type;
    void *kcd_cd_memcpy_f;
    unsigned long long kcd_cd_totalout_addr;
    unsigned long long kcd_cd_totalin_addr;
};
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_zs) == 0x0, "ks_kcdata_compress_descriptor.kcd_cd_zs offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_base) == 0x70, "ks_kcdata_compress_descriptor.kcd_cd_base offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_offset) == 0x78, "ks_kcdata_compress_descriptor.kcd_cd_offset offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_maxoffset) == 0x80, "ks_kcdata_compress_descriptor.kcd_cd_maxoffset offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_mark_begin) == 0x88, "ks_kcdata_compress_descriptor.kcd_cd_mark_begin offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_flags) == 0x90, "ks_kcdata_compress_descriptor.kcd_cd_flags offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_compression_type) == 0x98, "ks_kcdata_compress_descriptor.kcd_cd_compression_type offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_memcpy_f) == 0xa0, "ks_kcdata_compress_descriptor.kcd_cd_memcpy_f offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_totalout_addr) == 0xa8, "ks_kcdata_compress_descriptor.kcd_cd_totalout_addr offset");
_Static_assert(offsetof(struct ks_kcdata_compress_descriptor, kcd_cd_totalin_addr) == 0xb0, "ks_kcdata_compress_descriptor.kcd_cd_totalin_addr offset");

struct ks_kcdata_descriptor {
    unsigned int kcd_length;
    unsigned short kcd_flags;
    unsigned short kcd_user_flags;
    unsigned long long kcd_addr_begin;
    unsigned long long kcd_addr_end;
    struct ks_kcdata_compress_descriptor kcd_comp_d;
    unsigned int kcd_endalloced;
    struct ks_kcdata_descriptor **kcd_alloc_callback;
};
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_length) == 0x0, "ks_kcdata_descriptor.kcd_length offset");
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_flags) == 0x4, "ks_kcdata_descriptor.kcd_flags offset");
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_user_flags) == 0x6, "ks_kcdata_descriptor.kcd_user_flags offset");
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_addr_begin) == 0x8, "ks_kcdata_descriptor.kcd_addr_begin offset");
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_addr_end) == 0x10, "ks_kcdata_descriptor.kcd_addr_end offset");
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_comp_d) == 0x18, "ks_kcdata_descriptor.kcd_comp_d offset");
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_endalloced) == 0xd0, "ks_kcdata_descriptor.kcd_endalloced offset");
_Static_assert(offsetof(struct ks_kcdata_descriptor, kcd_alloc_callback) == 0xd8, "ks_kcdata_descriptor.kcd_alloc_callback offset");

struct ks_label {
    struct ks_label **l_owner;
    long l_perpolicy[7];
};
_Static_assert(offsetof(struct ks_label, l_owner) == 0x0, "ks_label.l_owner offset");
_Static_assert(offsetof(struct ks_label, l_perpolicy) == 0x8, "ks_label.l_perpolicy offset");

struct ks_ledger_entry_small {
    unsigned int les_flags;
    long long les_credit __attribute__((aligned(8)));
};
_Static_assert(offsetof(struct ks_ledger_entry_small, les_flags) == 0x0, "ks_ledger_entry_small.les_flags offset");
_Static_assert(offsetof(struct ks_ledger_entry_small, les_credit) == 0x8, "ks_ledger_entry_small.les_credit offset");

struct ks_os_refcnt {
    unsigned int ref_count;
};
_Static_assert(offsetof(struct ks_os_refcnt, ref_count) == 0x0, "ks_os_refcnt.ref_count offset");

struct ks_ledger {
    unsigned long long l_id;
    struct ks_os_refcnt l_refs;
    int l_size;
    struct ks_ledger_template *l_template;
    struct ks_ledger_entry_small l_entries[] __attribute__((aligned(8)));
};
_Static_assert(offsetof(struct ks_ledger, l_id) == 0x0, "ks_ledger.l_id offset");
_Static_assert(offsetof(struct ks_ledger, l_refs) == 0x8, "ks_ledger.l_refs offset");
_Static_assert(offsetof(struct ks_ledger, l_size) == 0xc, "ks_ledger.l_size offset");
_Static_assert(offsetof(struct ks_ledger, l_template) == 0x10, "ks_ledger.l_template offset");
_Static_assert(offsetof(struct ks_ledger, l_entries) == 0x18, "ks_ledger.l_entries offset");

struct ks_priority_queue_deadline_min {
    struct ks_priority_queue_entry_deadline *pq_root;
};
_Static_assert(offsetof(struct ks_priority_queue_deadline_min, pq_root) == 0x0, "ks_priority_queue_deadline_min.pq_root offset");

struct ks_priority_queue_entry_deadline {
    struct ks_priority_queue_entry_deadline *next;
    struct ks_priority_queue_entry_deadline *prev;
    long __key : 16; /* bit offset 128 */
    long child : 48; /* bit offset 144 */
    unsigned long long deadline;
};
_Static_assert(offsetof(struct ks_priority_queue_entry_deadline, next) == 0x0, "ks_priority_queue_entry_deadline.next offset");
_Static_assert(offsetof(struct ks_priority_queue_entry_deadline, prev) == 0x8, "ks_priority_queue_entry_deadline.prev offset");
_Static_assert(offsetof(struct ks_priority_queue_entry_deadline, deadline) == 0x18, "ks_priority_queue_entry_deadline.deadline offset");

struct ks_proc_platform_ro_data {
    unsigned int p_platform;
    unsigned int p_min_sdk;
    unsigned int p_sdk;
};
_Static_assert(offsetof(struct ks_proc_platform_ro_data, p_platform) == 0x0, "ks_proc_platform_ro_data.p_platform offset");
_Static_assert(offsetof(struct ks_proc_platform_ro_data, p_min_sdk) == 0x4, "ks_proc_platform_ro_data.p_min_sdk offset");
_Static_assert(offsetof(struct ks_proc_platform_ro_data, p_sdk) == 0x8, "ks_proc_platform_ro_data.p_sdk offset");

struct ks_proc_ro_data_p_ucred {
    volatile struct ks_ucred *__smr_ptr;
};
_Static_assert(offsetof(struct ks_proc_ro_data_p_ucred, __smr_ptr) == 0x0, "ks_proc_ro_data_p_ucred.__smr_ptr offset");

struct ks_proc_ro_data {
    unsigned long long p_uniqueid;
    int p_idversion;
    int p_orig_ppid;
    int p_orig_ppidversion;
    unsigned int p_csflags;
    struct ks_proc_ro_data_p_ucred p_ucred;
    unsigned char *syscall_filter_mask;
    struct ks_proc_platform_ro_data p_platform_data;
};
_Static_assert(offsetof(struct ks_proc_ro_data, p_uniqueid) == 0x0, "ks_proc_ro_data.p_uniqueid offset");
_Static_assert(offsetof(struct ks_proc_ro_data, p_idversion) == 0x8, "ks_proc_ro_data.p_idversion offset");
_Static_assert(offsetof(struct ks_proc_ro_data, p_orig_ppid) == 0xc, "ks_proc_ro_data.p_orig_ppid offset");
_Static_assert(offsetof(struct ks_proc_ro_data, p_orig_ppidversion) == 0x10, "ks_proc_ro_data.p_orig_ppidversion offset");
_Static_assert(offsetof(struct ks_proc_ro_data, p_csflags) == 0x14, "ks_proc_ro_data.p_csflags offset");
_Static_assert(offsetof(struct ks_proc_ro_data, p_ucred) == 0x18, "ks_proc_ro_data.p_ucred offset");
_Static_assert(offsetof(struct ks_proc_ro_data, syscall_filter_mask) == 0x20, "ks_proc_ro_data.syscall_filter_mask offset");
_Static_assert(offsetof(struct ks_proc_ro_data, p_platform_data) == 0x28, "ks_proc_ro_data.p_platform_data offset");

struct ks_task_filter_ro_data {
    unsigned char *mach_trap_filter_mask;
    unsigned char *mach_kobj_filter_mask;
};
_Static_assert(offsetof(struct ks_task_filter_ro_data, mach_trap_filter_mask) == 0x0, "ks_task_filter_ro_data.mach_trap_filter_mask offset");
_Static_assert(offsetof(struct ks_task_filter_ro_data, mach_kobj_filter_mask) == 0x8, "ks_task_filter_ro_data.mach_kobj_filter_mask offset");

struct ks_security_token_t {
    unsigned int val[2];
};
_Static_assert(offsetof(struct ks_security_token_t, val) == 0x0, "ks_security_token_t.val offset");

struct ks_task_token_ro_data {
    struct ks_security_token_t sec_token __attribute__((aligned(4)));
    struct ks_audit_token_t audit_token __attribute__((aligned(4)));
};
_Static_assert(offsetof(struct ks_task_token_ro_data, sec_token) == 0x0, "ks_task_token_ro_data.sec_token offset");
_Static_assert(offsetof(struct ks_task_token_ro_data, audit_token) == 0x8, "ks_task_token_ro_data.audit_token offset");

struct ks_task_ro_data {
    struct ks_task_token_ro_data task_tokens;
    struct ks_task_filter_ro_data task_filters;
    unsigned int t_flags_ro;
    unsigned char task_control_port_options;
};
_Static_assert(offsetof(struct ks_task_ro_data, task_tokens) == 0x0, "ks_task_ro_data.task_tokens offset");
_Static_assert(offsetof(struct ks_task_ro_data, task_filters) == 0x28, "ks_task_ro_data.task_filters offset");
_Static_assert(offsetof(struct ks_task_ro_data, t_flags_ro) == 0x38, "ks_task_ro_data.t_flags_ro offset");
_Static_assert(offsetof(struct ks_task_ro_data, task_control_port_options) == 0x3c, "ks_task_ro_data.task_control_port_options offset");

struct ks_proc_ro {
    struct ks_proc *pr_proc;
    struct ks_task *pr_task;
    struct ks_proc_ro_data proc_data;
    struct ks_task_ro_data task_data;
};
_Static_assert(offsetof(struct ks_proc_ro, pr_proc) == 0x0, "ks_proc_ro.pr_proc offset");
_Static_assert(offsetof(struct ks_proc_ro, pr_task) == 0x8, "ks_proc_ro.pr_task offset");
_Static_assert(offsetof(struct ks_proc_ro, proc_data) == 0x10, "ks_proc_ro.proc_data offset");
_Static_assert(offsetof(struct ks_proc_ro, task_data) == 0x48, "ks_proc_ro.task_data offset");

struct ks_rt_queue_pri_t {
    struct ks_queue_entry pri_queue;
    unsigned long long pri_earliest_deadline;
    int pri_count;
    unsigned int pri_constraint;
};
_Static_assert(offsetof(struct ks_rt_queue_pri_t, pri_queue) == 0x0, "ks_rt_queue_pri_t.pri_queue offset");
_Static_assert(offsetof(struct ks_rt_queue_pri_t, pri_earliest_deadline) == 0x10, "ks_rt_queue_pri_t.pri_earliest_deadline offset");
_Static_assert(offsetof(struct ks_rt_queue_pri_t, pri_count) == 0x18, "ks_rt_queue_pri_t.pri_count offset");
_Static_assert(offsetof(struct ks_rt_queue_pri_t, pri_constraint) == 0x1c, "ks_rt_queue_pri_t.pri_constraint offset");

struct ks_runq_stats {
    unsigned long long count_sum;
    unsigned long long last_change_timestamp;
};
_Static_assert(offsetof(struct ks_runq_stats, count_sum) == 0x0, "ks_runq_stats.count_sum offset");
_Static_assert(offsetof(struct ks_runq_stats, last_change_timestamp) == 0x8, "ks_runq_stats.last_change_timestamp offset");

struct ks_rt_queue {
    unsigned long long earliest_deadline;
    _Atomic int count;
    unsigned int constraint;
    _Atomic int ed_index;
    unsigned long long bitmap[1];
    struct ks_rt_queue_pri_t rt_queue_pri[31];
    struct ks_runq_stats runq_stats;
};
_Static_assert(offsetof(struct ks_rt_queue, earliest_deadline) == 0x0, "ks_rt_queue.earliest_deadline offset");
_Static_assert(offsetof(struct ks_rt_queue, count) == 0x8, "ks_rt_queue.count offset");
_Static_assert(offsetof(struct ks_rt_queue, constraint) == 0xc, "ks_rt_queue.constraint offset");
_Static_assert(offsetof(struct ks_rt_queue, ed_index) == 0x10, "ks_rt_queue.ed_index offset");
_Static_assert(offsetof(struct ks_rt_queue, bitmap) == 0x18, "ks_rt_queue.bitmap offset");
_Static_assert(offsetof(struct ks_rt_queue, rt_queue_pri) == 0x20, "ks_rt_queue.rt_queue_pri offset");
_Static_assert(offsetof(struct ks_rt_queue, runq_stats) == 0x400, "ks_rt_queue.runq_stats offset");

struct ks_run_queue {
    int highq;
    unsigned long long bitmap[2];
    int count;
    int urgency;
    struct ks_circle_queue_head queues[96];
    struct ks_runq_stats runq_stats;
};
_Static_assert(offsetof(struct ks_run_queue, highq) == 0x0, "ks_run_queue.highq offset");
_Static_assert(offsetof(struct ks_run_queue, bitmap) == 0x8, "ks_run_queue.bitmap offset");
_Static_assert(offsetof(struct ks_run_queue, count) == 0x18, "ks_run_queue.count offset");
_Static_assert(offsetof(struct ks_run_queue, urgency) == 0x1c, "ks_run_queue.urgency offset");
_Static_assert(offsetof(struct ks_run_queue, queues) == 0x20, "ks_run_queue.queues offset");
_Static_assert(offsetof(struct ks_run_queue, runq_stats) == 0x320, "ks_run_queue.runq_stats offset");

struct ks_sched_clutch_bucket_runq {
    int scbrq_highq;
    int scbrq_count;
    unsigned long long scbrq_bitmap[2];
    struct ks_circle_queue_head scbrq_queues[128];
};
_Static_assert(offsetof(struct ks_sched_clutch_bucket_runq, scbrq_highq) == 0x0, "ks_sched_clutch_bucket_runq.scbrq_highq offset");
_Static_assert(offsetof(struct ks_sched_clutch_bucket_runq, scbrq_count) == 0x4, "ks_sched_clutch_bucket_runq.scbrq_count offset");
_Static_assert(offsetof(struct ks_sched_clutch_bucket_runq, scbrq_bitmap) == 0x8, "ks_sched_clutch_bucket_runq.scbrq_bitmap offset");
_Static_assert(offsetof(struct ks_sched_clutch_bucket_runq, scbrq_queues) == 0x18, "ks_sched_clutch_bucket_runq.scbrq_queues offset");

struct ks_sched_clutch_root_bucket {
    unsigned char scrb_bucket;
    _Bool scrb_bound;
    _Bool scrb_starvation_avoidance;
    union {
        struct ks_sched_clutch_bucket_runq scrb_clutch_buckets;
        struct ks_run_queue scrb_bound_thread_runq;
    };
    struct ks_priority_queue_entry_deadline scrb_pqlink;
    unsigned long long scrb_warped_deadline;
    unsigned long long scrb_warp_remaining;
    unsigned long long scrb_starvation_ts;
};
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_bucket) == 0x0, "ks_sched_clutch_root_bucket.scrb_bucket offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_bound) == 0x1, "ks_sched_clutch_root_bucket.scrb_bound offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_starvation_avoidance) == 0x2, "ks_sched_clutch_root_bucket.scrb_starvation_avoidance offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_pqlink) == 0x420, "ks_sched_clutch_root_bucket.scrb_pqlink offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_warped_deadline) == 0x440, "ks_sched_clutch_root_bucket.scrb_warped_deadline offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_warp_remaining) == 0x448, "ks_sched_clutch_root_bucket.scrb_warp_remaining offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_starvation_ts) == 0x450, "ks_sched_clutch_root_bucket.scrb_starvation_ts offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_clutch_buckets) == 0x8, "ks_sched_clutch_root_bucket.scrb_clutch_buckets offset");
_Static_assert(offsetof(struct ks_sched_clutch_root_bucket, scrb_bound_thread_runq) == 0x8, "ks_sched_clutch_root_bucket.scrb_bound_thread_runq offset");

struct ks_sched_clutch_root {
    short scr_priority;
    unsigned short scr_thr_count;
    short scr_urgency;
    unsigned short scr_shared_rsrc_load_runnable[2];
    unsigned int scr_cluster_id;
    struct ks_processor_set *scr_pset;
    struct ks_queue_entry scr_clutch_buckets;
    struct ks_priority_queue_sched_max scr_foreign_buckets;
    unsigned long long scr_unbound_runnable_bitmap[1];
    unsigned long long scr_unbound_warp_available[1];
    unsigned long long scr_bound_runnable_bitmap[1];
    unsigned long long scr_bound_warp_available[1];
    struct ks_priority_queue_deadline_min scr_unbound_root_buckets;
    struct ks_priority_queue_deadline_min scr_bound_root_buckets;
    unsigned short scr_cumulative_run_count[6];
    struct ks_sched_clutch_root_bucket scr_unbound_buckets[6];
    struct ks_sched_clutch_root_bucket scr_bound_buckets[6];
};
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_priority) == 0x0, "ks_sched_clutch_root.scr_priority offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_thr_count) == 0x2, "ks_sched_clutch_root.scr_thr_count offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_urgency) == 0x4, "ks_sched_clutch_root.scr_urgency offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_shared_rsrc_load_runnable) == 0x6, "ks_sched_clutch_root.scr_shared_rsrc_load_runnable offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_cluster_id) == 0xc, "ks_sched_clutch_root.scr_cluster_id offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_pset) == 0x10, "ks_sched_clutch_root.scr_pset offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_clutch_buckets) == 0x18, "ks_sched_clutch_root.scr_clutch_buckets offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_foreign_buckets) == 0x28, "ks_sched_clutch_root.scr_foreign_buckets offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_unbound_runnable_bitmap) == 0x30, "ks_sched_clutch_root.scr_unbound_runnable_bitmap offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_unbound_warp_available) == 0x38, "ks_sched_clutch_root.scr_unbound_warp_available offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_bound_runnable_bitmap) == 0x40, "ks_sched_clutch_root.scr_bound_runnable_bitmap offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_bound_warp_available) == 0x48, "ks_sched_clutch_root.scr_bound_warp_available offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_unbound_root_buckets) == 0x50, "ks_sched_clutch_root.scr_unbound_root_buckets offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_bound_root_buckets) == 0x58, "ks_sched_clutch_root.scr_bound_root_buckets offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_cumulative_run_count) == 0x60, "ks_sched_clutch_root.scr_cumulative_run_count offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_unbound_buckets) == 0x70, "ks_sched_clutch_root.scr_unbound_buckets offset");
_Static_assert(offsetof(struct ks_sched_clutch_root, scr_bound_buckets) == 0x1a80, "ks_sched_clutch_root.scr_bound_buckets offset");

struct ks_pset_execution_time_t___anon_member_7 {
    unsigned long long pset_avg_thread_execution_time;
    unsigned long long pset_execution_time_last_update;
};
_Static_assert(offsetof(struct ks_pset_execution_time_t___anon_member_7, pset_avg_thread_execution_time) == 0x0, "ks_pset_execution_time_t___anon_member_7.pset_avg_thread_execution_time offset");
_Static_assert(offsetof(struct ks_pset_execution_time_t___anon_member_7, pset_execution_time_last_update) == 0x8, "ks_pset_execution_time_t___anon_member_7.pset_execution_time_last_update offset");

union ks_pset_execution_time_t {
    struct ks_pset_execution_time_t___anon_member_7 __anon_member_7;
    unsigned __int128 pset_execution_time_packed;
};
_Static_assert(offsetof(union ks_pset_execution_time_t, __anon_member_7) == 0x0, "ks_pset_execution_time_t.__anon_member_7 offset");
_Static_assert(offsetof(union ks_pset_execution_time_t, pset_execution_time_packed) == 0x0, "ks_pset_execution_time_t.pset_execution_time_packed offset");

struct ks_sched_clutch_edge___anon_member_8 {
    unsigned int sce_migration_allowed : 1; /* bit offset 0 */
    unsigned int sce_steal_allowed : 1; /* bit offset 1 */
    unsigned int _reserved : 30; /* bit offset 2 */
    unsigned int sce_migration_weight;
};
_Static_assert(offsetof(struct ks_sched_clutch_edge___anon_member_8, sce_migration_weight) == 0x4, "ks_sched_clutch_edge___anon_member_8.sce_migration_weight offset");

union ks_sched_clutch_edge {
    struct ks_sched_clutch_edge___anon_member_8 __anon_member_8;
    unsigned long long sce_edge_packed;
};
_Static_assert(offsetof(union ks_sched_clutch_edge, __anon_member_8) == 0x0, "ks_sched_clutch_edge.__anon_member_8 offset");
_Static_assert(offsetof(union ks_sched_clutch_edge, sce_edge_packed) == 0x0, "ks_sched_clutch_edge.sce_edge_packed offset");

union ks_sched_pset_search_order_t {
    unsigned char spso_search_order[1];
    unsigned __int128 spso_packed;
};
_Static_assert(offsetof(union ks_sched_pset_search_order_t, spso_search_order) == 0x0, "ks_sched_pset_search_order_t.spso_search_order offset");
_Static_assert(offsetof(union ks_sched_pset_search_order_t, spso_packed) == 0x0, "ks_sched_pset_search_order_t.spso_packed offset");

struct ks_processor_set {
    int pset_id;
    int online_processor_count;
    int cpu_set_low;
    int cpu_set_hi;
    int cpu_set_count;
    int last_chosen;
    unsigned long long pset_load_average[6];
    unsigned int pset_runnable_depth[6];
    unsigned long long pset_load_last_update;
    unsigned long long cpu_bitmask;
    unsigned long long recommended_bitmask;
    unsigned long long cpu_state_map[7];
    unsigned long long realtime_map;
    unsigned long long cpu_available_map;
    struct ks_lck_ticket_s sched_lock __attribute__((aligned(128)));
    struct ks_run_queue pset_runq;
    struct ks_rt_queue rt_runq;
    unsigned long long stealable_rt_threads_earliest_deadline;
    struct ks_sched_clutch_root pset_clutch_root;
    unsigned long long pending_AST_URGENT_cpu_mask;
    unsigned long long pending_AST_PREEMPT_cpu_mask;
    unsigned long long pending_deferred_AST_cpu_mask;
    unsigned long long pending_spill_cpu_mask;
    unsigned long long rt_pending_spill_cpu_mask;
    struct ks_ipc_port *pset_self;
    struct ks_ipc_port *pset_name_self;
    struct ks_processor_set *pset_list;
    struct ks_pset_node *node;
    unsigned int pset_cluster_id;
    unsigned int pset_cluster_type;
    int pset_type;
    unsigned long long cpu_running_foreign;
    unsigned long long cpu_running_cluster_shared_rsrc_thread[2];
    unsigned int cpu_running_buckets[8];
    unsigned long long foreign_psets[1];
    unsigned long long native_psets[1];
    unsigned long long local_psets[1];
    unsigned long long remote_psets[1];
    union ks_pset_execution_time_t pset_execution_time[6];
    unsigned long long pset_cluster_shared_rsrc_load[2];
    union ks_sched_clutch_edge sched_edges[2][6];
    union ks_sched_pset_search_order_t spill_search_order[6];
    unsigned char max_parallel_cores[6];
    unsigned char max_parallel_clusters[6];
    union ks_sched_clutch_edge sched_rt_edges[2];
    union ks_sched_pset_search_order_t sched_rt_spill_search_order;
    union ks_sched_pset_search_order_t sched_rt_steal_search_order;
    unsigned long long perfcontrol_cpu_preferred_bitmask;
    unsigned long long perfcontrol_cpu_migration_bitmask;
    int cpu_preferred_last_chosen;
};
_Static_assert(offsetof(struct ks_processor_set, pset_id) == 0x0, "ks_processor_set.pset_id offset");
_Static_assert(offsetof(struct ks_processor_set, online_processor_count) == 0x4, "ks_processor_set.online_processor_count offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_set_low) == 0x8, "ks_processor_set.cpu_set_low offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_set_hi) == 0xc, "ks_processor_set.cpu_set_hi offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_set_count) == 0x10, "ks_processor_set.cpu_set_count offset");
_Static_assert(offsetof(struct ks_processor_set, last_chosen) == 0x14, "ks_processor_set.last_chosen offset");
_Static_assert(offsetof(struct ks_processor_set, pset_load_average) == 0x18, "ks_processor_set.pset_load_average offset");
_Static_assert(offsetof(struct ks_processor_set, pset_runnable_depth) == 0x48, "ks_processor_set.pset_runnable_depth offset");
_Static_assert(offsetof(struct ks_processor_set, pset_load_last_update) == 0x60, "ks_processor_set.pset_load_last_update offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_bitmask) == 0x68, "ks_processor_set.cpu_bitmask offset");
_Static_assert(offsetof(struct ks_processor_set, recommended_bitmask) == 0x70, "ks_processor_set.recommended_bitmask offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_state_map) == 0x78, "ks_processor_set.cpu_state_map offset");
_Static_assert(offsetof(struct ks_processor_set, realtime_map) == 0xb0, "ks_processor_set.realtime_map offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_available_map) == 0xb8, "ks_processor_set.cpu_available_map offset");
_Static_assert(offsetof(struct ks_processor_set, sched_lock) == 0x100, "ks_processor_set.sched_lock offset");
_Static_assert(offsetof(struct ks_processor_set, pset_runq) == 0x110, "ks_processor_set.pset_runq offset");
_Static_assert(offsetof(struct ks_processor_set, rt_runq) == 0x440, "ks_processor_set.rt_runq offset");
_Static_assert(offsetof(struct ks_processor_set, stealable_rt_threads_earliest_deadline) == 0x850, "ks_processor_set.stealable_rt_threads_earliest_deadline offset");
_Static_assert(offsetof(struct ks_processor_set, pset_clutch_root) == 0x858, "ks_processor_set.pset_clutch_root offset");
_Static_assert(offsetof(struct ks_processor_set, pending_AST_URGENT_cpu_mask) == 0x3ce8, "ks_processor_set.pending_AST_URGENT_cpu_mask offset");
_Static_assert(offsetof(struct ks_processor_set, pending_AST_PREEMPT_cpu_mask) == 0x3cf0, "ks_processor_set.pending_AST_PREEMPT_cpu_mask offset");
_Static_assert(offsetof(struct ks_processor_set, pending_deferred_AST_cpu_mask) == 0x3cf8, "ks_processor_set.pending_deferred_AST_cpu_mask offset");
_Static_assert(offsetof(struct ks_processor_set, pending_spill_cpu_mask) == 0x3d00, "ks_processor_set.pending_spill_cpu_mask offset");
_Static_assert(offsetof(struct ks_processor_set, rt_pending_spill_cpu_mask) == 0x3d08, "ks_processor_set.rt_pending_spill_cpu_mask offset");
_Static_assert(offsetof(struct ks_processor_set, pset_self) == 0x3d10, "ks_processor_set.pset_self offset");
_Static_assert(offsetof(struct ks_processor_set, pset_name_self) == 0x3d18, "ks_processor_set.pset_name_self offset");
_Static_assert(offsetof(struct ks_processor_set, pset_list) == 0x3d20, "ks_processor_set.pset_list offset");
_Static_assert(offsetof(struct ks_processor_set, node) == 0x3d28, "ks_processor_set.node offset");
_Static_assert(offsetof(struct ks_processor_set, pset_cluster_id) == 0x3d30, "ks_processor_set.pset_cluster_id offset");
_Static_assert(offsetof(struct ks_processor_set, pset_cluster_type) == 0x3d34, "ks_processor_set.pset_cluster_type offset");
_Static_assert(offsetof(struct ks_processor_set, pset_type) == 0x3d38, "ks_processor_set.pset_type offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_running_foreign) == 0x3d40, "ks_processor_set.cpu_running_foreign offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_running_cluster_shared_rsrc_thread) == 0x3d48, "ks_processor_set.cpu_running_cluster_shared_rsrc_thread offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_running_buckets) == 0x3d58, "ks_processor_set.cpu_running_buckets offset");
_Static_assert(offsetof(struct ks_processor_set, foreign_psets) == 0x3d78, "ks_processor_set.foreign_psets offset");
_Static_assert(offsetof(struct ks_processor_set, native_psets) == 0x3d80, "ks_processor_set.native_psets offset");
_Static_assert(offsetof(struct ks_processor_set, local_psets) == 0x3d88, "ks_processor_set.local_psets offset");
_Static_assert(offsetof(struct ks_processor_set, remote_psets) == 0x3d90, "ks_processor_set.remote_psets offset");
_Static_assert(offsetof(struct ks_processor_set, pset_execution_time) == 0x3da0, "ks_processor_set.pset_execution_time offset");
_Static_assert(offsetof(struct ks_processor_set, pset_cluster_shared_rsrc_load) == 0x3e00, "ks_processor_set.pset_cluster_shared_rsrc_load offset");
_Static_assert(offsetof(struct ks_processor_set, sched_edges) == 0x3e10, "ks_processor_set.sched_edges offset");
_Static_assert(offsetof(struct ks_processor_set, spill_search_order) == 0x3e70, "ks_processor_set.spill_search_order offset");
_Static_assert(offsetof(struct ks_processor_set, max_parallel_cores) == 0x3ed0, "ks_processor_set.max_parallel_cores offset");
_Static_assert(offsetof(struct ks_processor_set, max_parallel_clusters) == 0x3ed6, "ks_processor_set.max_parallel_clusters offset");
_Static_assert(offsetof(struct ks_processor_set, sched_rt_edges) == 0x3ee0, "ks_processor_set.sched_rt_edges offset");
_Static_assert(offsetof(struct ks_processor_set, sched_rt_spill_search_order) == 0x3ef0, "ks_processor_set.sched_rt_spill_search_order offset");
_Static_assert(offsetof(struct ks_processor_set, sched_rt_steal_search_order) == 0x3f00, "ks_processor_set.sched_rt_steal_search_order offset");
_Static_assert(offsetof(struct ks_processor_set, perfcontrol_cpu_preferred_bitmask) == 0x3f10, "ks_processor_set.perfcontrol_cpu_preferred_bitmask offset");
_Static_assert(offsetof(struct ks_processor_set, perfcontrol_cpu_migration_bitmask) == 0x3f18, "ks_processor_set.perfcontrol_cpu_migration_bitmask offset");
_Static_assert(offsetof(struct ks_processor_set, cpu_preferred_last_chosen) == 0x3f20, "ks_processor_set.cpu_preferred_last_chosen offset");

struct ks_recount_metrics {
    unsigned long long rm_time_mach;
    unsigned long long rm_instructions;
    unsigned long long rm_cycles;
};
_Static_assert(offsetof(struct ks_recount_metrics, rm_time_mach) == 0x0, "ks_recount_metrics.rm_time_mach offset");
_Static_assert(offsetof(struct ks_recount_metrics, rm_instructions) == 0x8, "ks_recount_metrics.rm_instructions offset");
_Static_assert(offsetof(struct ks_recount_metrics, rm_cycles) == 0x10, "ks_recount_metrics.rm_cycles offset");

struct ks_recount_task {
    struct ks_recount_track *rtk_lifetime;
    struct ks_recount_usage *rtk_terminated;
};
_Static_assert(offsetof(struct ks_recount_task, rtk_lifetime) == 0x0, "ks_recount_task.rtk_lifetime offset");
_Static_assert(offsetof(struct ks_recount_task, rtk_terminated) == 0x8, "ks_recount_task.rtk_terminated offset");

struct ks_recount_usage {
    struct ks_recount_metrics ru_metrics[3];
    unsigned long long ru_energy_nj;
};
_Static_assert(offsetof(struct ks_recount_usage, ru_metrics) == 0x0, "ks_recount_usage.ru_metrics offset");
_Static_assert(offsetof(struct ks_recount_usage, ru_energy_nj) == 0x48, "ks_recount_usage.ru_energy_nj offset");

struct ks_recount_track {
    unsigned int rt_pad;
    unsigned int rt_sync;
    struct ks_recount_usage rt_usage;
};
_Static_assert(offsetof(struct ks_recount_track, rt_pad) == 0x0, "ks_recount_track.rt_pad offset");
_Static_assert(offsetof(struct ks_recount_track, rt_sync) == 0x4, "ks_recount_track.rt_sync offset");
_Static_assert(offsetof(struct ks_recount_track, rt_usage) == 0x8, "ks_recount_track.rt_usage offset");

struct ks_task_restartable_range_t {
    unsigned long long location;
    unsigned short length;
    unsigned short recovery_offs;
    unsigned int flags;
};
_Static_assert(offsetof(struct ks_task_restartable_range_t, location) == 0x0, "ks_task_restartable_range_t.location offset");
_Static_assert(offsetof(struct ks_task_restartable_range_t, length) == 0x8, "ks_task_restartable_range_t.length offset");
_Static_assert(offsetof(struct ks_task_restartable_range_t, recovery_offs) == 0xa, "ks_task_restartable_range_t.recovery_offs offset");
_Static_assert(offsetof(struct ks_task_restartable_range_t, flags) == 0xc, "ks_task_restartable_range_t.flags offset");

struct ks_restartable_ranges {
    struct ks_queue_entry rr_link;
    struct ks_os_refcnt rr_ref;
    unsigned int rr_count;
    unsigned int rr_hash;
    struct ks_task_restartable_range_t rr_ranges[64];
};
_Static_assert(offsetof(struct ks_restartable_ranges, rr_link) == 0x0, "ks_restartable_ranges.rr_link offset");
_Static_assert(offsetof(struct ks_restartable_ranges, rr_ref) == 0x10, "ks_restartable_ranges.rr_ref offset");
_Static_assert(offsetof(struct ks_restartable_ranges, rr_count) == 0x14, "ks_restartable_ranges.rr_count offset");
_Static_assert(offsetof(struct ks_restartable_ranges, rr_hash) == 0x18, "ks_restartable_ranges.rr_hash offset");
_Static_assert(offsetof(struct ks_restartable_ranges, rr_ranges) == 0x20, "ks_restartable_ranges.rr_ranges offset");

struct ks_task_effective_policy {
    unsigned long long tep_darwinbg : 1; /* bit offset 0 */
    unsigned long long tep_lowpri_cpu : 1; /* bit offset 1 */
    unsigned long long tep_io_tier : 2; /* bit offset 2 */
    unsigned long long tep_io_passive : 1; /* bit offset 4 */
    unsigned long long tep_all_sockets_bg : 1; /* bit offset 5 */
    unsigned long long tep_new_sockets_bg : 1; /* bit offset 6 */
    unsigned long long tep_bg_iotier : 2; /* bit offset 7 */
    unsigned long long tep_terminated : 1; /* bit offset 9 */
    unsigned long long tep_qos_ui_is_urgent : 1; /* bit offset 10 */
    unsigned long long tep_latency_qos : 3; /* bit offset 11 */
    unsigned long long tep_through_qos : 3; /* bit offset 14 */
    unsigned long long tep_tal_engaged : 1; /* bit offset 17 */
    unsigned long long tep_watchers_bg : 1; /* bit offset 18 */
    unsigned long long tep_sup_active : 1; /* bit offset 19 */
    unsigned long long tep_role : 4; /* bit offset 20 */
    unsigned long long tep_suppressed_cpu : 1; /* bit offset 24 */
    unsigned long long tep_sfi_managed : 1; /* bit offset 25 */
    unsigned long long tep_live_donor : 1; /* bit offset 26 */
    unsigned long long tep_qos_clamp : 3; /* bit offset 27 */
    unsigned long long tep_qos_ceiling : 3; /* bit offset 30 */
    unsigned long long tep_promote_above_task : 1; /* bit offset 33 */
    unsigned long long tep_coalition_bg : 1; /* bit offset 34 */
    unsigned long long tep_runaway_mitigation : 1; /* bit offset 35 */
    unsigned long long tep_reserved : 28; /* bit offset 36 */
};

struct ks_task_pend_token___anon_member_26___anon_member_27 {
    unsigned int tpt_update_sockets : 1; /* bit offset 0 */
    unsigned int tpt_update_timers : 1; /* bit offset 1 */
    unsigned int tpt_update_watchers : 1; /* bit offset 2 */
    unsigned int tpt_update_live_donor : 1; /* bit offset 3 */
    unsigned int tpt_update_coal_sfi : 1; /* bit offset 4 */
    unsigned int tpt_update_throttle : 1; /* bit offset 5 */
    unsigned int tpt_update_thread_sfi : 1; /* bit offset 6 */
    unsigned int tpt_force_recompute_pri : 1; /* bit offset 7 */
    unsigned int tpt_update_tg_ui_flag : 1; /* bit offset 8 */
    unsigned int tpt_update_turnstile : 1; /* bit offset 9 */
    unsigned int tpt_update_tg_app_flag : 1; /* bit offset 10 */
    unsigned int tpt_update_game_mode : 1; /* bit offset 11 */
    unsigned int tpt_update_carplay_mode : 1; /* bit offset 12 */
    unsigned int tpt_update_appnap : 1; /* bit offset 13 */
};

struct ks_task_pend_token {
    union {
        struct ks_task_pend_token___anon_member_26___anon_member_27 __anon_member_27;
        unsigned int tpt_value;
    };
};
_Static_assert(offsetof(struct ks_task_pend_token, __anon_member_27) == 0x0, "ks_task_pend_token.__anon_member_27 offset");
_Static_assert(offsetof(struct ks_task_pend_token, tpt_value) == 0x0, "ks_task_pend_token.tpt_value offset");

struct ks_task_requested_policy {
    unsigned long long trp_int_darwinbg : 1; /* bit offset 0 */
    unsigned long long trp_ext_darwinbg : 1; /* bit offset 1 */
    unsigned long long trp_int_iotier : 2; /* bit offset 2 */
    unsigned long long trp_ext_iotier : 2; /* bit offset 4 */
    unsigned long long trp_int_iopassive : 1; /* bit offset 6 */
    unsigned long long trp_ext_iopassive : 1; /* bit offset 7 */
    unsigned long long trp_bg_iotier : 2; /* bit offset 8 */
    unsigned long long trp_terminated : 1; /* bit offset 10 */
    unsigned long long trp_base_latency_qos : 3; /* bit offset 11 */
    unsigned long long trp_base_through_qos : 3; /* bit offset 14 */
    unsigned long long trp_apptype : 3; /* bit offset 17 */
    unsigned long long trp_boosted : 1; /* bit offset 20 */
    unsigned long long trp_role : 5; /* bit offset 21 */
    unsigned long long trp_over_latency_qos : 3; /* bit offset 26 */
    unsigned long long trp_over_through_qos : 3; /* bit offset 29 */
    unsigned long long trp_sfi_managed : 1; /* bit offset 32 */
    unsigned long long trp_qos_clamp : 3; /* bit offset 33 */
    unsigned long long trp_sup_active : 1; /* bit offset 36 */
    unsigned long long trp_sup_lowpri_cpu : 1; /* bit offset 37 */
    unsigned long long trp_sup_timer : 3; /* bit offset 38 */
    unsigned long long trp_sup_disk : 1; /* bit offset 41 */
    unsigned long long trp_sup_throughput : 3; /* bit offset 42 */
    unsigned long long trp_sup_cpu : 1; /* bit offset 45 */
    unsigned long long trp_sup_bg_sockets : 1; /* bit offset 46 */
    unsigned long long trp_runaway_mitigation : 1; /* bit offset 47 */
    unsigned long long trp_reserved : 16; /* bit offset 48 */
};

struct ks_task_security_config___anon_member_33___anon_member_34 {
    unsigned short hardened_heap : 1; /* bit offset 0 */
    unsigned short tpro : 1; /* bit offset 1 */
    unsigned short reserved : 1; /* bit offset 2 */
    unsigned short platform_restrictions_version : 3; /* bit offset 3 */
    unsigned short script_restrictions : 1; /* bit offset 6 */
    unsigned short ipc_containment_vessel : 1; /* bit offset 7 */
    unsigned short guard_objects : 1; /* bit offset 8 */
    unsigned char hardened_process_version;
};
_Static_assert(offsetof(struct ks_task_security_config___anon_member_33___anon_member_34, hardened_process_version) == 0x2, "ks_task_security_config___anon_member_33___anon_member_34.hardened_process_version offset");

struct ks_task_security_config {
    union {
        struct ks_task_security_config___anon_member_33___anon_member_34 __anon_member_34;
        unsigned int value;
    };
};
_Static_assert(offsetof(struct ks_task_security_config, __anon_member_34) == 0x0, "ks_task_security_config.__anon_member_34 offset");
_Static_assert(offsetof(struct ks_task_security_config, value) == 0x0, "ks_task_security_config.value offset");

struct ks_task_writes_counters {
    unsigned long long task_immediate_writes;
    unsigned long long task_deferred_writes;
    unsigned long long task_invalidated_writes;
    unsigned long long task_metadata_writes;
};
_Static_assert(offsetof(struct ks_task_writes_counters, task_immediate_writes) == 0x0, "ks_task_writes_counters.task_immediate_writes offset");
_Static_assert(offsetof(struct ks_task_writes_counters, task_deferred_writes) == 0x8, "ks_task_writes_counters.task_deferred_writes offset");
_Static_assert(offsetof(struct ks_task_writes_counters, task_invalidated_writes) == 0x10, "ks_task_writes_counters.task_invalidated_writes offset");
_Static_assert(offsetof(struct ks_task_writes_counters, task_metadata_writes) == 0x18, "ks_task_writes_counters.task_metadata_writes offset");

struct ks_vm_extmod_statistics {
    long long task_for_pid_count;
    long long task_for_pid_caller_count;
    long long thread_creation_count;
    long long thread_creation_caller_count;
    long long thread_set_state_count;
    long long thread_set_state_caller_count;
};
_Static_assert(offsetof(struct ks_vm_extmod_statistics, task_for_pid_count) == 0x0, "ks_vm_extmod_statistics.task_for_pid_count offset");
_Static_assert(offsetof(struct ks_vm_extmod_statistics, task_for_pid_caller_count) == 0x8, "ks_vm_extmod_statistics.task_for_pid_caller_count offset");
_Static_assert(offsetof(struct ks_vm_extmod_statistics, thread_creation_count) == 0x10, "ks_vm_extmod_statistics.thread_creation_count offset");
_Static_assert(offsetof(struct ks_vm_extmod_statistics, thread_creation_caller_count) == 0x18, "ks_vm_extmod_statistics.thread_creation_caller_count offset");
_Static_assert(offsetof(struct ks_vm_extmod_statistics, thread_set_state_count) == 0x20, "ks_vm_extmod_statistics.thread_set_state_count offset");
_Static_assert(offsetof(struct ks_vm_extmod_statistics, thread_set_state_caller_count) == 0x28, "ks_vm_extmod_statistics.thread_set_state_caller_count offset");

struct ks_task {
    struct ks_lck_mtx_s lock;
    struct ks_os_refcnt ref_count;
    _Bool active;
    _Bool ipc_active;
    _Bool halting;
    _Bool message_app_suspended;
    unsigned int vtimers;
    unsigned int loadTag;
    unsigned long long task_uniqueid;
    struct ks__vm_map *map;
    struct ks_queue_entry tasks;
    struct ks_task_watchports *watchports;
    void *returnwait_inheritor;
    struct ks_queue_entry threads;
    struct ks_restartable_ranges *t_rr_ranges;
    struct ks_processor_set *pset_hint;
    struct ks_affinity_space *affinity_space;
    int thread_count;
    unsigned int active_thread_count;
    int suspend_count;
    int user_stop_count;
    int legacy_stop_count;
    short priority;
    short max_priority;
    int importance;
    unsigned long long total_runnable_time;
    struct ks_recount_task tk_recount;
    struct ks_lck_mtx_s itk_lock_data;
    struct ks_ipc_port *itk_task_ports[4];
    struct ks_ipc_port *itk_settable_self;
    struct ks_exception_action exc_actions[14];
    struct ks_hardened_exception_action hardened_exception_action;
    struct ks_ipc_port *itk_host;
    struct ks_ipc_port *itk_bootstrap;
    struct ks_ipc_port *itk_debug_control;
    struct ks_ipc_port *itk_task_access;
    struct ks_ipc_port *itk_resume;
    struct ks_ipc_port *itk_registered[3];
    struct ks_ipc_port **itk_dyld_notify;
    struct ks_ipc_space *itk_space;
    struct ks_ledger *ledger;
    struct ks_queue_entry semaphore_list;
    int semaphores_owned;
    unsigned int priv_flags;
    void *task_debug;
    unsigned long long rop_pid;
    unsigned long long jop_pid;
    unsigned char disable_user_jop;
    unsigned char has_jitbox;
    unsigned long long jitbox_version;
    unsigned long long jitbox_start;
    unsigned long long jitbox_size;
    int jitbox_enabled;
    struct ks_arm64_uexc_region_t uexc;
    _Bool preserve_x18;
    _Bool uses_1ghz_timebase;
    unsigned long long *faults;
    unsigned long long *pageins;
    unsigned long long *cow_faults;
    unsigned long long *messages_sent;
    unsigned long long *messages_received;
    unsigned int decompressions;
    unsigned int syscalls_mach;
    unsigned int syscalls_unix;
    unsigned int c_switch;
    unsigned int p_switch;
    unsigned int ps_switch;
    struct ks_proc_ro *bsd_info_ro;
    struct ks_kcdata_descriptor *corpse_info;
    unsigned long long crashed_thread_id;
    struct ks_queue_entry corpse_tasks;
    struct ks_label *crash_label;
    unsigned int t_flags;
    unsigned int t_procflags;
    unsigned long long all_image_info_addr;
    unsigned long long all_image_info_size;
    unsigned int t_kpc;
    unsigned char t_gpu_role;
    _Bool pidsuspended;
    _Bool frozen;
    _Bool changing_freeze_state;
    _Bool is_large_corpse;
    unsigned short policy_ru_cpu : 4; /* bit offset 8648 */
    unsigned short policy_ru_cpu_ext : 4; /* bit offset 8652 */
    unsigned short applied_ru_cpu : 4; /* bit offset 8656 */
    unsigned short applied_ru_cpu_ext : 4; /* bit offset 8660 */
    unsigned char rusage_cpu_flags;
    unsigned char rusage_cpu_percentage;
    unsigned char rusage_cpu_perthr_percentage;
    unsigned char t_returnwaitflags;
    _Bool shared_region_auth_remapped;
    char *shared_region_id;
    struct ks_vm_shared_region *shared_region;
    unsigned long long rusage_cpu_interval;
    unsigned long long rusage_cpu_perthr_interval;
    unsigned long long rusage_cpu_deadline;
    struct ks_thread_call *rusage_cpu_callt;
    struct ks_queue_entry task_watchers;
    int num_taskwatchers;
    int watchapplying;
    struct ks_bank_task *bank_context;
    struct ks_ipc_importance_task *task_imp_base;
    struct ks_vm_extmod_statistics extmod_statistics __attribute__((aligned(8)));
    struct ks_task_requested_policy requested_policy;
    struct ks_task_effective_policy effective_policy;
    struct ks_task_pend_token pended_coalition_changes;
    unsigned int low_mem_notified_warn : 1; /* bit offset 9952 */
    unsigned int low_mem_notified_critical : 1; /* bit offset 9953 */
    unsigned int purged_memory_warn : 1; /* bit offset 9954 */
    unsigned int purged_memory_critical : 1; /* bit offset 9955 */
    unsigned int low_mem_privileged_listener : 1; /* bit offset 9956 */
    unsigned int mem_notify_reserved : 27; /* bit offset 9957 */
    unsigned int memlimit_flags;
    struct ks_io_stat_info *task_io_stats;
    struct ks_task_writes_counters task_writes_counters_internal;
    struct ks_task_writes_counters task_writes_counters_external;
    struct ks__cpu_time_qos_stats cpu_time_eqos_stats;
    struct ks__cpu_time_qos_stats cpu_time_rqos_stats;
    unsigned int task_timer_wakeups_bin_1;
    unsigned int task_timer_wakeups_bin_2;
    unsigned long long task_gpu_ns;
    unsigned char task_can_transfer_memory_ownership;
    unsigned char task_objects_disowning;
    unsigned char task_objects_disowned;
    int task_volatile_objects;
    int task_nonvolatile_objects;
    int task_owned_objects;
    struct ks_queue_entry task_objq;
    struct ks_lck_mtx_s task_objq_lock;
    unsigned int task_thread_limit : 16; /* bit offset 12032 */
    unsigned int task_legacy_footprint : 1; /* bit offset 12048 */
    unsigned int task_extra_footprint_limit : 1; /* bit offset 12049 */
    unsigned int task_ios13extended_footprint_limit : 1; /* bit offset 12050 */
    unsigned int task_region_footprint : 1; /* bit offset 12051 */
    unsigned int task_region_info_flags : 1; /* bit offset 12052 */
    unsigned int task_has_crossed_thread_limit : 1; /* bit offset 12053 */
    unsigned int task_rr_in_flight : 1; /* bit offset 12054 */
    unsigned int task_jetsam_realtime_audio : 1; /* bit offset 12055 */
    struct ks_coalition *coalition[2];
    struct ks_queue_entry task_coalition[2];
    unsigned long long dispatchqueue_offset;
    void *hv_task_target;
    unsigned int task_exc_guard;
    unsigned long long mach_header_vm_address;
    struct ks_queue_entry io_user_clients;
    int donates_own_pages;
    unsigned int task_shared_region_slide;
    unsigned long long task_fs_metadata_writes;
    unsigned char task_shared_region_uuid[16];
    unsigned long long memstat_dirty_start;
    struct ks__vmobject_list_output_ *corpse_vmobject_list;
    unsigned long long corpse_vmobject_list_size;
    struct ks_vm_deferred_reclamation_metadata_s *deferred_reclamation_metadata;
    unsigned long long task_cs_auxiliary_info;
    struct ks_task_security_config security_config;
};
_Static_assert(offsetof(struct ks_task, lock) == 0x0, "ks_task.lock offset");
_Static_assert(offsetof(struct ks_task, ref_count) == 0x10, "ks_task.ref_count offset");
_Static_assert(offsetof(struct ks_task, active) == 0x14, "ks_task.active offset");
_Static_assert(offsetof(struct ks_task, ipc_active) == 0x15, "ks_task.ipc_active offset");
_Static_assert(offsetof(struct ks_task, halting) == 0x16, "ks_task.halting offset");
_Static_assert(offsetof(struct ks_task, message_app_suspended) == 0x17, "ks_task.message_app_suspended offset");
_Static_assert(offsetof(struct ks_task, vtimers) == 0x18, "ks_task.vtimers offset");
_Static_assert(offsetof(struct ks_task, loadTag) == 0x1c, "ks_task.loadTag offset");
_Static_assert(offsetof(struct ks_task, task_uniqueid) == 0x20, "ks_task.task_uniqueid offset");
_Static_assert(offsetof(struct ks_task, map) == 0x28, "ks_task.map offset");
_Static_assert(offsetof(struct ks_task, tasks) == 0x30, "ks_task.tasks offset");
_Static_assert(offsetof(struct ks_task, watchports) == 0x40, "ks_task.watchports offset");
_Static_assert(offsetof(struct ks_task, returnwait_inheritor) == 0x48, "ks_task.returnwait_inheritor offset");
_Static_assert(offsetof(struct ks_task, threads) == 0x50, "ks_task.threads offset");
_Static_assert(offsetof(struct ks_task, t_rr_ranges) == 0x60, "ks_task.t_rr_ranges offset");
_Static_assert(offsetof(struct ks_task, pset_hint) == 0x68, "ks_task.pset_hint offset");
_Static_assert(offsetof(struct ks_task, affinity_space) == 0x70, "ks_task.affinity_space offset");
_Static_assert(offsetof(struct ks_task, thread_count) == 0x78, "ks_task.thread_count offset");
_Static_assert(offsetof(struct ks_task, active_thread_count) == 0x7c, "ks_task.active_thread_count offset");
_Static_assert(offsetof(struct ks_task, suspend_count) == 0x80, "ks_task.suspend_count offset");
_Static_assert(offsetof(struct ks_task, user_stop_count) == 0x84, "ks_task.user_stop_count offset");
_Static_assert(offsetof(struct ks_task, legacy_stop_count) == 0x88, "ks_task.legacy_stop_count offset");
_Static_assert(offsetof(struct ks_task, priority) == 0x8c, "ks_task.priority offset");
_Static_assert(offsetof(struct ks_task, max_priority) == 0x8e, "ks_task.max_priority offset");
_Static_assert(offsetof(struct ks_task, importance) == 0x90, "ks_task.importance offset");
_Static_assert(offsetof(struct ks_task, total_runnable_time) == 0x98, "ks_task.total_runnable_time offset");
_Static_assert(offsetof(struct ks_task, tk_recount) == 0xa0, "ks_task.tk_recount offset");
_Static_assert(offsetof(struct ks_task, itk_lock_data) == 0xb0, "ks_task.itk_lock_data offset");
_Static_assert(offsetof(struct ks_task, itk_task_ports) == 0xc0, "ks_task.itk_task_ports offset");
_Static_assert(offsetof(struct ks_task, itk_settable_self) == 0xe0, "ks_task.itk_settable_self offset");
_Static_assert(offsetof(struct ks_task, exc_actions) == 0xe8, "ks_task.exc_actions offset");
_Static_assert(offsetof(struct ks_task, hardened_exception_action) == 0x2a8, "ks_task.hardened_exception_action offset");
_Static_assert(offsetof(struct ks_task, itk_host) == 0x2d0, "ks_task.itk_host offset");
_Static_assert(offsetof(struct ks_task, itk_bootstrap) == 0x2d8, "ks_task.itk_bootstrap offset");
_Static_assert(offsetof(struct ks_task, itk_debug_control) == 0x2e0, "ks_task.itk_debug_control offset");
_Static_assert(offsetof(struct ks_task, itk_task_access) == 0x2e8, "ks_task.itk_task_access offset");
_Static_assert(offsetof(struct ks_task, itk_resume) == 0x2f0, "ks_task.itk_resume offset");
_Static_assert(offsetof(struct ks_task, itk_registered) == 0x2f8, "ks_task.itk_registered offset");
_Static_assert(offsetof(struct ks_task, itk_dyld_notify) == 0x310, "ks_task.itk_dyld_notify offset");
_Static_assert(offsetof(struct ks_task, itk_space) == 0x318, "ks_task.itk_space offset");
_Static_assert(offsetof(struct ks_task, ledger) == 0x320, "ks_task.ledger offset");
_Static_assert(offsetof(struct ks_task, semaphore_list) == 0x328, "ks_task.semaphore_list offset");
_Static_assert(offsetof(struct ks_task, semaphores_owned) == 0x338, "ks_task.semaphores_owned offset");
_Static_assert(offsetof(struct ks_task, priv_flags) == 0x33c, "ks_task.priv_flags offset");
_Static_assert(offsetof(struct ks_task, task_debug) == 0x340, "ks_task.task_debug offset");
_Static_assert(offsetof(struct ks_task, rop_pid) == 0x348, "ks_task.rop_pid offset");
_Static_assert(offsetof(struct ks_task, jop_pid) == 0x350, "ks_task.jop_pid offset");
_Static_assert(offsetof(struct ks_task, disable_user_jop) == 0x358, "ks_task.disable_user_jop offset");
_Static_assert(offsetof(struct ks_task, has_jitbox) == 0x359, "ks_task.has_jitbox offset");
_Static_assert(offsetof(struct ks_task, jitbox_version) == 0x360, "ks_task.jitbox_version offset");
_Static_assert(offsetof(struct ks_task, jitbox_start) == 0x368, "ks_task.jitbox_start offset");
_Static_assert(offsetof(struct ks_task, jitbox_size) == 0x370, "ks_task.jitbox_size offset");
_Static_assert(offsetof(struct ks_task, jitbox_enabled) == 0x378, "ks_task.jitbox_enabled offset");
_Static_assert(offsetof(struct ks_task, uexc) == 0x380, "ks_task.uexc offset");
_Static_assert(offsetof(struct ks_task, preserve_x18) == 0x3a0, "ks_task.preserve_x18 offset");
_Static_assert(offsetof(struct ks_task, uses_1ghz_timebase) == 0x3a1, "ks_task.uses_1ghz_timebase offset");
_Static_assert(offsetof(struct ks_task, faults) == 0x3a8, "ks_task.faults offset");
_Static_assert(offsetof(struct ks_task, pageins) == 0x3b0, "ks_task.pageins offset");
_Static_assert(offsetof(struct ks_task, cow_faults) == 0x3b8, "ks_task.cow_faults offset");
_Static_assert(offsetof(struct ks_task, messages_sent) == 0x3c0, "ks_task.messages_sent offset");
_Static_assert(offsetof(struct ks_task, messages_received) == 0x3c8, "ks_task.messages_received offset");
_Static_assert(offsetof(struct ks_task, decompressions) == 0x3d0, "ks_task.decompressions offset");
_Static_assert(offsetof(struct ks_task, syscalls_mach) == 0x3d4, "ks_task.syscalls_mach offset");
_Static_assert(offsetof(struct ks_task, syscalls_unix) == 0x3d8, "ks_task.syscalls_unix offset");
_Static_assert(offsetof(struct ks_task, c_switch) == 0x3dc, "ks_task.c_switch offset");
_Static_assert(offsetof(struct ks_task, p_switch) == 0x3e0, "ks_task.p_switch offset");
_Static_assert(offsetof(struct ks_task, ps_switch) == 0x3e4, "ks_task.ps_switch offset");
_Static_assert(offsetof(struct ks_task, bsd_info_ro) == 0x3e8, "ks_task.bsd_info_ro offset");
_Static_assert(offsetof(struct ks_task, corpse_info) == 0x3f0, "ks_task.corpse_info offset");
_Static_assert(offsetof(struct ks_task, crashed_thread_id) == 0x3f8, "ks_task.crashed_thread_id offset");
_Static_assert(offsetof(struct ks_task, corpse_tasks) == 0x400, "ks_task.corpse_tasks offset");
_Static_assert(offsetof(struct ks_task, crash_label) == 0x410, "ks_task.crash_label offset");
_Static_assert(offsetof(struct ks_task, t_flags) == 0x418, "ks_task.t_flags offset");
_Static_assert(offsetof(struct ks_task, t_procflags) == 0x41c, "ks_task.t_procflags offset");
_Static_assert(offsetof(struct ks_task, all_image_info_addr) == 0x420, "ks_task.all_image_info_addr offset");
_Static_assert(offsetof(struct ks_task, all_image_info_size) == 0x428, "ks_task.all_image_info_size offset");
_Static_assert(offsetof(struct ks_task, t_kpc) == 0x430, "ks_task.t_kpc offset");
_Static_assert(offsetof(struct ks_task, t_gpu_role) == 0x434, "ks_task.t_gpu_role offset");
_Static_assert(offsetof(struct ks_task, pidsuspended) == 0x435, "ks_task.pidsuspended offset");
_Static_assert(offsetof(struct ks_task, frozen) == 0x436, "ks_task.frozen offset");
_Static_assert(offsetof(struct ks_task, changing_freeze_state) == 0x437, "ks_task.changing_freeze_state offset");
_Static_assert(offsetof(struct ks_task, is_large_corpse) == 0x438, "ks_task.is_large_corpse offset");
_Static_assert(offsetof(struct ks_task, rusage_cpu_flags) == 0x43b, "ks_task.rusage_cpu_flags offset");
_Static_assert(offsetof(struct ks_task, rusage_cpu_percentage) == 0x43c, "ks_task.rusage_cpu_percentage offset");
_Static_assert(offsetof(struct ks_task, rusage_cpu_perthr_percentage) == 0x43d, "ks_task.rusage_cpu_perthr_percentage offset");
_Static_assert(offsetof(struct ks_task, t_returnwaitflags) == 0x43e, "ks_task.t_returnwaitflags offset");
_Static_assert(offsetof(struct ks_task, shared_region_auth_remapped) == 0x43f, "ks_task.shared_region_auth_remapped offset");
_Static_assert(offsetof(struct ks_task, shared_region_id) == 0x440, "ks_task.shared_region_id offset");
_Static_assert(offsetof(struct ks_task, shared_region) == 0x448, "ks_task.shared_region offset");
_Static_assert(offsetof(struct ks_task, rusage_cpu_interval) == 0x450, "ks_task.rusage_cpu_interval offset");
_Static_assert(offsetof(struct ks_task, rusage_cpu_perthr_interval) == 0x458, "ks_task.rusage_cpu_perthr_interval offset");
_Static_assert(offsetof(struct ks_task, rusage_cpu_deadline) == 0x460, "ks_task.rusage_cpu_deadline offset");
_Static_assert(offsetof(struct ks_task, rusage_cpu_callt) == 0x468, "ks_task.rusage_cpu_callt offset");
_Static_assert(offsetof(struct ks_task, task_watchers) == 0x470, "ks_task.task_watchers offset");
_Static_assert(offsetof(struct ks_task, num_taskwatchers) == 0x480, "ks_task.num_taskwatchers offset");
_Static_assert(offsetof(struct ks_task, watchapplying) == 0x484, "ks_task.watchapplying offset");
_Static_assert(offsetof(struct ks_task, bank_context) == 0x488, "ks_task.bank_context offset");
_Static_assert(offsetof(struct ks_task, task_imp_base) == 0x490, "ks_task.task_imp_base offset");
_Static_assert(offsetof(struct ks_task, extmod_statistics) == 0x498, "ks_task.extmod_statistics offset");
_Static_assert(offsetof(struct ks_task, requested_policy) == 0x4c8, "ks_task.requested_policy offset");
_Static_assert(offsetof(struct ks_task, effective_policy) == 0x4d0, "ks_task.effective_policy offset");
_Static_assert(offsetof(struct ks_task, pended_coalition_changes) == 0x4d8, "ks_task.pended_coalition_changes offset");
_Static_assert(offsetof(struct ks_task, memlimit_flags) == 0x4e0, "ks_task.memlimit_flags offset");
_Static_assert(offsetof(struct ks_task, task_io_stats) == 0x4e8, "ks_task.task_io_stats offset");
_Static_assert(offsetof(struct ks_task, task_writes_counters_internal) == 0x4f0, "ks_task.task_writes_counters_internal offset");
_Static_assert(offsetof(struct ks_task, task_writes_counters_external) == 0x510, "ks_task.task_writes_counters_external offset");
_Static_assert(offsetof(struct ks_task, cpu_time_eqos_stats) == 0x530, "ks_task.cpu_time_eqos_stats offset");
_Static_assert(offsetof(struct ks_task, cpu_time_rqos_stats) == 0x568, "ks_task.cpu_time_rqos_stats offset");
_Static_assert(offsetof(struct ks_task, task_timer_wakeups_bin_1) == 0x5a0, "ks_task.task_timer_wakeups_bin_1 offset");
_Static_assert(offsetof(struct ks_task, task_timer_wakeups_bin_2) == 0x5a4, "ks_task.task_timer_wakeups_bin_2 offset");
_Static_assert(offsetof(struct ks_task, task_gpu_ns) == 0x5a8, "ks_task.task_gpu_ns offset");
_Static_assert(offsetof(struct ks_task, task_can_transfer_memory_ownership) == 0x5b0, "ks_task.task_can_transfer_memory_ownership offset");
_Static_assert(offsetof(struct ks_task, task_objects_disowning) == 0x5b1, "ks_task.task_objects_disowning offset");
_Static_assert(offsetof(struct ks_task, task_objects_disowned) == 0x5b2, "ks_task.task_objects_disowned offset");
_Static_assert(offsetof(struct ks_task, task_volatile_objects) == 0x5b4, "ks_task.task_volatile_objects offset");
_Static_assert(offsetof(struct ks_task, task_nonvolatile_objects) == 0x5b8, "ks_task.task_nonvolatile_objects offset");
_Static_assert(offsetof(struct ks_task, task_owned_objects) == 0x5bc, "ks_task.task_owned_objects offset");
_Static_assert(offsetof(struct ks_task, task_objq) == 0x5c0, "ks_task.task_objq offset");
_Static_assert(offsetof(struct ks_task, task_objq_lock) == 0x5d0, "ks_task.task_objq_lock offset");
_Static_assert(offsetof(struct ks_task, coalition) == 0x5e8, "ks_task.coalition offset");
_Static_assert(offsetof(struct ks_task, task_coalition) == 0x5f8, "ks_task.task_coalition offset");
_Static_assert(offsetof(struct ks_task, dispatchqueue_offset) == 0x618, "ks_task.dispatchqueue_offset offset");
_Static_assert(offsetof(struct ks_task, hv_task_target) == 0x620, "ks_task.hv_task_target offset");
_Static_assert(offsetof(struct ks_task, task_exc_guard) == 0x628, "ks_task.task_exc_guard offset");
_Static_assert(offsetof(struct ks_task, mach_header_vm_address) == 0x630, "ks_task.mach_header_vm_address offset");
_Static_assert(offsetof(struct ks_task, io_user_clients) == 0x638, "ks_task.io_user_clients offset");
_Static_assert(offsetof(struct ks_task, donates_own_pages) == 0x648, "ks_task.donates_own_pages offset");
_Static_assert(offsetof(struct ks_task, task_shared_region_slide) == 0x64c, "ks_task.task_shared_region_slide offset");
_Static_assert(offsetof(struct ks_task, task_fs_metadata_writes) == 0x650, "ks_task.task_fs_metadata_writes offset");
_Static_assert(offsetof(struct ks_task, task_shared_region_uuid) == 0x658, "ks_task.task_shared_region_uuid offset");
_Static_assert(offsetof(struct ks_task, memstat_dirty_start) == 0x668, "ks_task.memstat_dirty_start offset");
_Static_assert(offsetof(struct ks_task, corpse_vmobject_list) == 0x670, "ks_task.corpse_vmobject_list offset");
_Static_assert(offsetof(struct ks_task, corpse_vmobject_list_size) == 0x678, "ks_task.corpse_vmobject_list_size offset");
_Static_assert(offsetof(struct ks_task, deferred_reclamation_metadata) == 0x680, "ks_task.deferred_reclamation_metadata offset");
_Static_assert(offsetof(struct ks_task, task_cs_auxiliary_info) == 0x688, "ks_task.task_cs_auxiliary_info offset");
_Static_assert(offsetof(struct ks_task, security_config) == 0x690, "ks_task.security_config offset");

struct ks_task_watchport_elem {
    struct ks_task *twe_task;
    struct ks_ipc_port *twe_port;
    struct ks_ipc_port *twe_pdrequest;
};
_Static_assert(offsetof(struct ks_task_watchport_elem, twe_task) == 0x0, "ks_task_watchport_elem.twe_task offset");
_Static_assert(offsetof(struct ks_task_watchport_elem, twe_port) == 0x8, "ks_task_watchport_elem.twe_port offset");
_Static_assert(offsetof(struct ks_task_watchport_elem, twe_pdrequest) == 0x10, "ks_task_watchport_elem.twe_pdrequest offset");

struct ks_task_watchports {
    struct ks_os_refcnt tw_refcount;
    struct ks_task *tw_task;
    struct ks_thread *tw_thread;
    unsigned int tw_elem_array_count;
    struct ks_task_watchport_elem tw_elem[];
};
_Static_assert(offsetof(struct ks_task_watchports, tw_refcount) == 0x0, "ks_task_watchports.tw_refcount offset");
_Static_assert(offsetof(struct ks_task_watchports, tw_task) == 0x8, "ks_task_watchports.tw_task offset");
_Static_assert(offsetof(struct ks_task_watchports, tw_thread) == 0x10, "ks_task_watchports.tw_thread offset");
_Static_assert(offsetof(struct ks_task_watchports, tw_elem_array_count) == 0x18, "ks_task_watchports.tw_elem_array_count offset");
_Static_assert(offsetof(struct ks_task_watchports, tw_elem) == 0x20, "ks_task_watchports.tw_elem offset");

struct ks_vm_deferred_reclamation_metadata_s_vdrm_list {
    struct ks_vm_deferred_reclamation_metadata_s *tqe_next;
    struct ks_vm_deferred_reclamation_metadata_s **tqe_prev;
};
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s_vdrm_list, tqe_next) == 0x0, "ks_vm_deferred_reclamation_metadata_s_vdrm_list.tqe_next offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s_vdrm_list, tqe_prev) == 0x8, "ks_vm_deferred_reclamation_metadata_s_vdrm_list.tqe_prev offset");

struct ks_vm_deferred_reclamation_metadata_s {
    struct ks_vm_deferred_reclamation_metadata_s_vdrm_list vdrm_list;
    struct ks_lck_mtx_s vdrm_lock;
    struct ks_gate vdrm_gate;
    struct ks_task *vdrm_task;
    int vdrm_pid;
    struct ks__vm_map *vdrm_map;
    struct ks_os_refcnt vdrm_refcnt;
    unsigned long long vdrm_ring_addr;
    unsigned long long vdrm_ring_size;
    unsigned int vdrm_buffer_len;
    unsigned long long vdrm_reclaimed_at;
    unsigned int vdrm_waiters;
    unsigned long long vdrm_last_sample_abs;
    unsigned long vdrm_kernel_bytes_reclaimed;
    unsigned long long vdrm_reclaimable_bytes_last;
    unsigned long long vdrm_reclaimable_bytes_wma;
    unsigned char vdrm_is_registered : 1; /* bit offset 1280 */
    unsigned char __unused1 : 7; /* bit offset 1281 */
};
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_list) == 0x0, "ks_vm_deferred_reclamation_metadata_s.vdrm_list offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_lock) == 0x10, "ks_vm_deferred_reclamation_metadata_s.vdrm_lock offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_gate) == 0x20, "ks_vm_deferred_reclamation_metadata_s.vdrm_gate offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_task) == 0x38, "ks_vm_deferred_reclamation_metadata_s.vdrm_task offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_pid) == 0x40, "ks_vm_deferred_reclamation_metadata_s.vdrm_pid offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_map) == 0x48, "ks_vm_deferred_reclamation_metadata_s.vdrm_map offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_refcnt) == 0x50, "ks_vm_deferred_reclamation_metadata_s.vdrm_refcnt offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_ring_addr) == 0x58, "ks_vm_deferred_reclamation_metadata_s.vdrm_ring_addr offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_ring_size) == 0x60, "ks_vm_deferred_reclamation_metadata_s.vdrm_ring_size offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_buffer_len) == 0x68, "ks_vm_deferred_reclamation_metadata_s.vdrm_buffer_len offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_reclaimed_at) == 0x70, "ks_vm_deferred_reclamation_metadata_s.vdrm_reclaimed_at offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_waiters) == 0x78, "ks_vm_deferred_reclamation_metadata_s.vdrm_waiters offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_last_sample_abs) == 0x80, "ks_vm_deferred_reclamation_metadata_s.vdrm_last_sample_abs offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_kernel_bytes_reclaimed) == 0x88, "ks_vm_deferred_reclamation_metadata_s.vdrm_kernel_bytes_reclaimed offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_reclaimable_bytes_last) == 0x90, "ks_vm_deferred_reclamation_metadata_s.vdrm_reclaimable_bytes_last offset");
_Static_assert(offsetof(struct ks_vm_deferred_reclamation_metadata_s, vdrm_reclaimable_bytes_wma) == 0x98, "ks_vm_deferred_reclamation_metadata_s.vdrm_reclaimable_bytes_wma offset");

struct ks_vm_shared_region {
    unsigned int sr_ref_count;
    unsigned int sr_slide;
    struct ks_queue_entry sr_q;
    void *sr_root_dir;
    int sr_cpu_type;
    int sr_cpu_subtype;
    struct ks_ipc_port *sr_mem_entry;
    struct ks__vm_map *sr_config_map;
    unsigned long long sr_first_mapping;
    unsigned long long sr_base_address;
    unsigned long long sr_size;
    unsigned long long sr_pmap_nesting_start;
    unsigned long long sr_pmap_nesting_size;
    struct ks_thread_call *sr_timer_call;
    unsigned char sr_uuid[16];
    unsigned char sr_page_shift;
    struct ks_thread *sr_mapping_in_progress;
    struct ks_thread *sr_slide_in_progress;
    _Bool sr_64bit;
    _Bool sr_persists;
    _Bool sr_uuid_copied;
    _Bool sr_stale;
    _Bool sr_driverkit;
    _Bool sr_reslide;
    unsigned int sr_num_auth_section;
    unsigned int sr_next_auth_section;
    struct ks_vm_shared_region_slide_info **sr_auth_section;
    unsigned int sr_rsr_version;
    unsigned long long sr_install_time;
    unsigned int sr_id;
    unsigned int sr_images_count;
    struct ks_dyld_uuid_info_64 *sr_images;
    unsigned char sr_aot_uuid[16];
    int sr_aot_uuid_copied;
    unsigned long long sr_aot_mapping;
};
_Static_assert(offsetof(struct ks_vm_shared_region, sr_ref_count) == 0x0, "ks_vm_shared_region.sr_ref_count offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_slide) == 0x4, "ks_vm_shared_region.sr_slide offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_q) == 0x8, "ks_vm_shared_region.sr_q offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_root_dir) == 0x18, "ks_vm_shared_region.sr_root_dir offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_cpu_type) == 0x20, "ks_vm_shared_region.sr_cpu_type offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_cpu_subtype) == 0x24, "ks_vm_shared_region.sr_cpu_subtype offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_mem_entry) == 0x28, "ks_vm_shared_region.sr_mem_entry offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_config_map) == 0x30, "ks_vm_shared_region.sr_config_map offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_first_mapping) == 0x38, "ks_vm_shared_region.sr_first_mapping offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_base_address) == 0x40, "ks_vm_shared_region.sr_base_address offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_size) == 0x48, "ks_vm_shared_region.sr_size offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_pmap_nesting_start) == 0x50, "ks_vm_shared_region.sr_pmap_nesting_start offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_pmap_nesting_size) == 0x58, "ks_vm_shared_region.sr_pmap_nesting_size offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_timer_call) == 0x60, "ks_vm_shared_region.sr_timer_call offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_uuid) == 0x68, "ks_vm_shared_region.sr_uuid offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_page_shift) == 0x78, "ks_vm_shared_region.sr_page_shift offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_mapping_in_progress) == 0x80, "ks_vm_shared_region.sr_mapping_in_progress offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_slide_in_progress) == 0x88, "ks_vm_shared_region.sr_slide_in_progress offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_64bit) == 0x90, "ks_vm_shared_region.sr_64bit offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_persists) == 0x91, "ks_vm_shared_region.sr_persists offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_uuid_copied) == 0x92, "ks_vm_shared_region.sr_uuid_copied offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_stale) == 0x93, "ks_vm_shared_region.sr_stale offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_driverkit) == 0x94, "ks_vm_shared_region.sr_driverkit offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_reslide) == 0x95, "ks_vm_shared_region.sr_reslide offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_num_auth_section) == 0x98, "ks_vm_shared_region.sr_num_auth_section offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_next_auth_section) == 0x9c, "ks_vm_shared_region.sr_next_auth_section offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_auth_section) == 0xa0, "ks_vm_shared_region.sr_auth_section offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_rsr_version) == 0xa8, "ks_vm_shared_region.sr_rsr_version offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_install_time) == 0xb0, "ks_vm_shared_region.sr_install_time offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_id) == 0xb8, "ks_vm_shared_region.sr_id offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_images_count) == 0xbc, "ks_vm_shared_region.sr_images_count offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_images) == 0xc0, "ks_vm_shared_region.sr_images offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_aot_uuid) == 0xc8, "ks_vm_shared_region.sr_aot_uuid offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_aot_uuid_copied) == 0xd8, "ks_vm_shared_region.sr_aot_uuid_copied offset");
_Static_assert(offsetof(struct ks_vm_shared_region, sr_aot_mapping) == 0xe0, "ks_vm_shared_region.sr_aot_mapping offset");

typedef unsigned char ks_Byte;
typedef unsigned char ks_Bytef;
typedef unsigned int ks___darwin_natural_t;
typedef int ks___darwin_pid_t;
typedef unsigned char ks___darwin_uuid_t[16];
typedef int ks___int32_t;
typedef struct ks___smrq_slink_t ks___smrq_slink_t;
typedef void **ks_alloc_func;
typedef struct ks_arm64_uexc_region_t ks_arm64_uexc_region_t;
typedef struct ks_audit_token_t ks_audit_token_t;
typedef unsigned long long ks_bitmap_t;
typedef int ks_boolean_t;
typedef struct ks_circle_queue_head ks_circle_queue_head_t;
typedef int ks_cluster_type_t;
typedef struct ks_coalition *ks_coalition_t;
typedef unsigned long long *ks_counter_t;
typedef int ks_cpu_subtype_t;
typedef int ks_cpu_type_t;
typedef unsigned long long ks_cpumap_t;
typedef unsigned char ks_darwin_gpu_role_t;
typedef int ks_exception_behavior_t;
typedef unsigned int ks_exception_mask_t;
typedef void *ks_free_func;
typedef struct ks_gate ks_gate_t;
typedef union ks_hw_lck_ticket_s ks_hw_lck_ticket_t;
typedef short ks_int16_t;
typedef int ks_int32_t;
typedef long long ks_int64_t;
typedef int ks_integer_t;
typedef struct ks_io_stat_info *ks_io_stat_info_t;
typedef unsigned int ks_ipc_entry_num_t;
typedef struct ks_ipc_entry_table *ks_ipc_entry_table_t;
typedef struct ks_ipc_importance_task *ks_ipc_importance_task_t;
typedef unsigned long long ks_ipc_label_t;
typedef unsigned int ks_ipc_object_bits_t;
typedef unsigned char ks_ipc_object_state_t;
typedef unsigned char ks_ipc_object_type_t;
typedef struct ks_ipc_port_request_table *ks_ipc_port_request_table_t;
typedef struct ks_ipc_port *ks_ipc_port_t;
typedef unsigned long ks_ipc_port_timestamp_t;
typedef unsigned int ks_ipc_table_elems_t;
typedef unsigned char ks_is_telemetry_t;
typedef unsigned long long ks_kcd_cd_flag_t;
typedef unsigned long long ks_kcd_compression_type_t;
typedef struct ks_kcdata_descriptor *ks_kcdata_descriptor_t;
typedef union ks_lck_mtx_state ks_lck_mtx_state_t;
typedef struct ks_lck_mtx_s ks_lck_mtx_t;
typedef struct ks_lck_rw_s ks_lck_rw_t;
typedef union ks_lck_rw_word_t ks_lck_rw_word_t;
typedef struct ks_lck_spin_s ks_lck_spin_t;
typedef struct ks_lck_ticket_s ks_lck_ticket_t;
typedef long long ks_ledger_amount_t;
typedef struct ks_ledger *ks_ledger_t;
typedef unsigned int ks_mach_port_mscount_t;
typedef unsigned int ks_mach_port_name_t;
typedef unsigned int ks_mach_port_rights_t;
typedef unsigned int ks_mach_port_seqno_t;
typedef unsigned long long ks_mach_vm_address_t;
typedef unsigned long long ks_mach_vm_offset_t;
typedef unsigned int ks_mach_vm_reclaim_count_t;
typedef unsigned long long ks_mach_vm_size_t;
typedef unsigned int ks_mach_voucher_attr_value_reference_t;
typedef unsigned int ks_natural_t;
typedef unsigned int ks_os_ref_atomic_t;
typedef unsigned int ks_os_ref_count_t;
typedef struct ks_os_refcnt ks_os_refcnt_t;
typedef int ks_pid_t;
typedef struct ks_pmap *ks_pmap_t;
typedef struct ks_processor_set *ks_processor_set_t;
typedef unsigned int ks_pset_cluster_type_t;
typedef union ks_pset_execution_time_t ks_pset_execution_time_t;
typedef unsigned char ks_pset_id_t;
typedef struct ks_pset_node *ks_pset_node_t;
typedef struct ks_queue_entry ks_queue_chain_t;
typedef struct ks_queue_entry *ks_queue_entry_t;
typedef struct ks_queue_entry ks_queue_head_t;
typedef struct ks_queue_entry *ks_queue_t;
typedef struct ks_rt_queue_pri_t ks_rt_queue_pri_t;
typedef unsigned long long *ks_scalable_counter_t;
typedef unsigned int ks_sched_bucket_t;
typedef union ks_sched_clutch_edge ks_sched_clutch_edge;
typedef union ks_sched_pset_search_order_t ks_sched_pset_search_order_t;
typedef struct ks_security_token_t ks_security_token_t;
typedef struct ks_lck_spin_s ks_simple_lock_data_t;
typedef unsigned long ks_size_t;
typedef void *ks_smr_cb_t;
typedef unsigned char ks_task_control_port_options_t;
typedef unsigned int ks_task_exc_guard_behavior_t;
typedef unsigned int ks_task_memlimit_flags_t;
typedef struct ks_task_restartable_range_t ks_task_restartable_range_t;
typedef struct ks_task_security_config ks_task_security_config_s;
typedef struct ks_task *ks_task_t;
typedef struct ks_thread_call *ks_thread_call_t;
typedef int ks_thread_state_flavor_t;
typedef struct ks_thread *ks_thread_t;
typedef void *ks_turnstile_inheritor_t;
typedef unsigned int ks_uInt;
typedef unsigned long ks_uLong;
typedef unsigned short ks_u_int16_t;
typedef unsigned int ks_u_int32_t;
typedef unsigned long long ks_u_int64_t;
typedef unsigned char ks_u_int8_t;
typedef unsigned short ks_uint16_t;
typedef unsigned int ks_uint32_t;
typedef unsigned long long ks_uint64_t;
typedef unsigned char ks_uint8_t;
typedef unsigned int ks_uint_t;
typedef unsigned long ks_uintptr_t;
typedef unsigned long long ks_user_addr_t;
typedef struct ks_lck_spin_s ks_usimple_lock_data_t;
typedef unsigned char ks_uuid_t[16];
typedef struct ks_vm_deferred_reclamation_metadata_s *ks_vm_deferred_reclamation_metadata_t;
typedef struct ks_vm_extmod_statistics ks_vm_extmod_statistics_data_t;
typedef struct ks_vm_map_entry *ks_vm_map_entry_t;
typedef unsigned long long ks_vm_map_offset_t;
typedef const void *ks_vm_map_serial_t;
typedef unsigned long long ks_vm_map_size_t;
typedef struct ks__vm_map *ks_vm_map_t;
typedef unsigned long long ks_vm_object_id_t;
typedef struct ks__vm_object_query_data_ ks_vm_object_query_data_t;
typedef struct ks_vm_shared_region_slide_info *ks_vm_shared_region_slide_info_t;
typedef struct ks__vmobject_list_output_ *ks_vmobject_list_output_t;
typedef void *ks_voidpf;
typedef unsigned int ks_waitq_flags_t;
typedef struct ks_waitq_link_list_entry ks_waitq_link_list_t;
typedef unsigned int ks_waitq_type_t;
typedef struct ks_z_stream_s ks_z_stream;

#endif /* _KS_STRUCTS_TASK_H */
